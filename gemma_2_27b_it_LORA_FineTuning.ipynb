{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "kDdY2Xatywjg"
            },
            "source": [
                "\n",
                "\n",
                "# **Finetuning ```google/gemma-2-27b-it``` model using ```LORA``` on ```SAMSum dataset``` (abstractive dialogue summaries)**\n",
                "\n",
                "\n",
                "\n",
                "*   **Author:** ```Pratik Vyas```\n",
                "*   **Task:** ```Summarization```\n",
                "*   **Pretrained model:** [gemma-2-27b-it]( https://huggingface.co/google/gemma-2-27b-it )\n",
                "*   **Dataset:** [SAMSum]( https://paperswithcode.com/dataset/samsum-corpus )\n",
                "*   **Evaluation Matrix:** ```Rouge score```\n",
                "*   **Finetuning Metrics:** [gemma-2-27b-it Finetuning Metrics](https://github.com/Git-PratikVyas/Finetuning-LORA/blob/main/FinetuningMetrics/gemma_2_27b_it_Analyse_finetuning_Metrics.ipynb)\n",
                "*   **Finetuned model at Huggingface hub:** [Prat/gemma-2-27b-it_ft_summarizer_v3](https://huggingface.co/Prat/gemma-2-27b-it-ft-summarizer-v3)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1lISpHHuLAdD"
            },
            "source": [
                "# **Import Libs**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "collapsed": true,
                "id": "SD6A_C-YXhE2",
                "jupyter": {
                    "outputs_hidden": true
                },
                "outputId": "8b85f085-60bc-46ed-b4a7-c67bc2337f0c"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m336.3/336.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.7/365.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
                        "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.6/536.6 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
                        "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2023.10.0 which is incompatible.\n",
                        "trl 0.12.2 requires datasets>=2.21.0, but you have datasets 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
                        "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
                        "trl 0.12.2 requires datasets>=2.21.0, but you have datasets 2.17.0 which is incompatible.\n",
                        "trl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n",
                        "\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m364.4/364.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.5/233.5 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
                        "  Building wheel for GPUtil (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
                    ]
                }
            ],
            "source": [
                "!pip3 install -q -U accelerate\n",
                "!pip3 install -q -U bitsandbytes\n",
                "!pip3 install -q -U peft\n",
                "!pip3 install -q -U trl\n",
                "!pip3 install -q -U datasets==2.17.0\n",
                "!pip3 install -q -U transformers\n",
                "!pip install -q rouge_score\n",
                "!pip install -q optuna\n",
                "!pip install -q --upgrade torch\n",
                "!pip3 install -q -U wandb\n",
                "!pip install -q accelerate\n",
                "!pip install -q GPUtil"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "Hy0GBC_tWYS0"
            },
            "outputs": [],
            "source": [
                "from peft import LoraConfig\n",
                "from datasets import load_dataset\n",
                "from datasets import load_metric\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "import transformers\n",
                "from trl import SFTTrainer\n",
                "from rouge_score import rouge_scorer\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
                "from google.colab import userdata"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "id": "rb9mDO0iWt1v"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "os.environ[\"HF_TOKEN\"] = \"HF_KEY\"\n",
                "os.environ[\"WB_KEY\"] = \"WB_KEY\""
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "p_tN-wYlKdi3"
            },
            "source": [
                "# **Load tokenizer**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 145,
                    "referenced_widgets": [
                        "dd4cb8b653444ecc9cbdd1c1111604a6",
                        "f279f593e36749608da958af562ae152",
                        "3c9a0b7caccd48388ea9a4413e589da9",
                        "49889eef57de4f988f0ea9480f1ee8c2",
                        "a1a267b65b11452ebba85d0bb8d69dbb",
                        "5b40d275551541c5bc9dfee66fa485df",
                        "f35ba085ff944d45bf41f83edb6eeb31",
                        "ff36f7873e074e1e952fb5c4cad0fa78",
                        "8b0821b841c4457882bdbaa4e4351e61",
                        "cec44c480d194af18f25127cd07858a1",
                        "79fa18c346994d36b3a2433a06e7bc97",
                        "86432403ea9a4d40bd9146e197a3661d",
                        "b3ac00736b084632acf3699c1d42830c",
                        "ff0e68e300cb447f924be9e63a3ffcad",
                        "9087689f4f0541688c218c7a070bbbeb",
                        "10163ee0d96f407c97bfc1d7cf817c1c",
                        "d3af1c7d063a42c887c527aeeda5f829",
                        "7d5b2682d4c04f29888d1bb9f346d115",
                        "fab1d413b7fe46ed8910dded9a5aac5b",
                        "235e6ec5d8af449290f18f41c9ebd055",
                        "fa664cacb6fb4eb49cb2f61d11afb56f",
                        "1cca3b428ee448a4ac002a9599fb9ba9",
                        "31c4d831ac934e4b98d32a323c170be9",
                        "837ec8c2d2494bd4b599cd8bb0d60a5e",
                        "e1a243d7e7884e71afcec4c9a08fe4f4",
                        "e8aa8ac0c50442238c23d20dc69ac036",
                        "23ac8922a10e40e2a38d1a70da664ca2",
                        "ecab0e0bfb8046fe91cd926b8d747591",
                        "640bddc8126a4fe79300b56610bb07d6",
                        "7a73c0214e2f487aa3516e68e5f7f7f5",
                        "976346a8a1934021817d295c0fb14699",
                        "a26cb8cae4a14674a8d72b62ed884545",
                        "492cd7227e174abdbb4d4e8931a6574b",
                        "2abb9d9ca40c4375906ad3f7f7316177",
                        "37e3d8bd6a2c4f27aea8c5ceec8cc1d2",
                        "8151e1d3b8c34167a9e0dfe233d97bd6",
                        "c7a3068a9da8494ebed89a128c24ed91",
                        "eebed6e1ab0d463a989898ef7e073d7e",
                        "4ab9852a7abe41e5914e43e06bd3397c",
                        "41546532712749838641a0c172f886bb",
                        "8323e5f921c84df7959c4ab4d0addec1",
                        "2cf6d3c441604393bb6366678fdb7986",
                        "dae6f666772e434b9c52b11eb8ed4b89",
                        "305c01dc690541e5ba76c6465598e75d"
                    ]
                },
                "id": "fvQLkb9bW2PE",
                "outputId": "2609ac5f-a183-4712-bce3-a135a185c735"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "dd4cb8b653444ecc9cbdd1c1111604a6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "86432403ea9a4d40bd9146e197a3661d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "31c4d831ac934e4b98d32a323c170be9",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "2abb9d9ca40c4375906ad3f7f7316177",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# load a pre-trained tokenizer from the Hugging Face Model Hub, with authentication for the Hugging Face API token\n",
                "\n",
                "model_id = \"google/gemma-2-27b-it\"\n",
                "\n",
                "new_model = \"gemma-2-27b-it_ft\"\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"HF_TOKEN\"])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "W9iK1mibgRk7"
            },
            "source": [
                "# **Load Dataset**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 767,
                    "referenced_widgets": [
                        "c98b566c74534d29ab81736c54efdeba",
                        "61e4b4acd66e4c1a883f9fdfe19c2b63",
                        "80a6be95301845088badf285f0f3d305",
                        "a126d84d3f15446b9a1700a4b4483c46",
                        "7c079c7cb4704dd4bb80dc433947d93f",
                        "83dd20da3bb74e87b9cb392c92fbf5cb",
                        "9336db28a1cb40cdbb735a1db8087ef6",
                        "62fafc9c14554c87967ec235813ba8fe",
                        "d4f3ad0671f84b6f9948152644db14b9",
                        "2ef04e86283a47df8477d27b65df0e1e",
                        "b358e1696ef94ee19695e53bb97e2959",
                        "a05d9561e2ad4d28a1d05abd59a749fc",
                        "3ec665c1f39041a187144674d2857d87",
                        "89819313705f4bdb88a334728818aeaa",
                        "172ed61060d44cfd9d6909d2f825e655",
                        "81f8f158a395408d80a3abf5e3497aba",
                        "ee3c5bc3ca4842548da73760eed1e77e",
                        "6e8e049ecc934840a25262e8e0c1c0d8",
                        "28b815e2f4f440b88d3332d6b764c7e3",
                        "96455babec564b1ea2c7544a7db92709",
                        "ad4340dcb52b4df6810a8829a843aae8",
                        "efb1f75c3d4b41d6b6ce76b01aca7bd2",
                        "72a72460b6964842b383ee77f7b1cd89",
                        "b6c331be4b5b497db7e7ae4e8c3b5f5f",
                        "d66ecc56ddfb464f84f37e9975ae8636",
                        "061b04a22a9e465294ebce8eb7428df3",
                        "9d4cb0f152e6407793f5b1848424bd2b",
                        "e4dff1e1c7a64e95bfcf092590cfa5d6",
                        "afabf30df6af4b5d8b4dbc228ef5aacf",
                        "a4515ea0ec5f41dcab3c468b4320a929",
                        "1fa84b7a767d483184b35b3f4c5c3bee",
                        "b1cee402a43c457a8894073e4cf7f66c",
                        "3d07087121a94742bf6b567eca1a2dc8",
                        "c31d5b6d478741e5980942cad16a18c8",
                        "ba0eb20aa4d642f6b246775e2104042a",
                        "60cf5c586ebf488e99f21a7ee67489db",
                        "aa6077f10eaa4ec3a2d7a9abdbba21ae",
                        "84f8084ffa1d4422b9c20be6ad0d21c2",
                        "d360d273420a4b668c33566cc12e1649",
                        "2f4b26c3ab3a44169e7d36174c23f9d4",
                        "76c549f2538e46299420a58fcfab2599",
                        "30fd6651de6f4d7db8fbbb2a0ddeaa05",
                        "41e33ddb533d4c1d8b643b8f487b99d4",
                        "0761c7334be946389ce327c4cc4b07bf",
                        "026958fd812d48d0aca6d9cadba915dc",
                        "01dd9b50ed9c4031bb49f5b0acb9b24e",
                        "570f4fadf5564868946b4300ac4413d9",
                        "30cc969f3a9a4371a92424be2f6fa7e7",
                        "8f06529435c54b3dbbb28335db6f65f1",
                        "e0b0dd18a6bb4dc2988e2be3d5322071",
                        "23e1b0fc8723486597eab0472decae80",
                        "c4ca998f70fb493d888897858e985431",
                        "5bdc8dfe13434f938979192233a82186",
                        "f46e3d1ed54049778d694c0e02b25f31",
                        "0e793e2be3df46eda30b4bb4a98267ef",
                        "f3a4471e62c74856ac31ca75d2410228",
                        "252f9143297b4479b8efc879d164843a",
                        "510ef5da27064d2395bc7dac67f86551",
                        "8d2db38ca7cb4fe4859231087d8c07a8",
                        "4f5b2771fe0c46259f42b6b9ccfc39f3",
                        "360ba57b87294757bd33ee512a2014c3",
                        "5b2fc30c8943412ba6d32ebce3358b3f",
                        "763bd40de04742b7b3b0681b5e65cfed",
                        "688cb7da0c7e4dc29d4492956373a510",
                        "65d15392a87e435a82fdde6100b0b143",
                        "e30862b82fd54921a6fd439562a35ccc"
                    ]
                },
                "id": "DjS8Hwy0gN07",
                "outputId": "ba771e73-bc6b-4c18-c415-3691e16d9fd1"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
                        "\u001b[?25h"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
                        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
                        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
                        "You will be able to reuse this secret in all of your notebooks.\n",
                        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
                        "  warnings.warn(\n",
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c98b566c74534d29ab81736c54efdeba",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script:   0%|          | 0.00/3.36k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a05d9561e2ad4d28a1d05abd59a749fc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading readme:   0%|          | 0.00/7.04k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "72a72460b6964842b383ee77f7b1cd89",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading data:   0%|          | 0.00/2.94M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c31d5b6d478741e5980942cad16a18c8",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating train split:   0%|          | 0/14732 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "026958fd812d48d0aca6d9cadba915dc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "f3a4471e62c74856ac31ca75d2410228",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "DatasetDict({\n",
                        "    train: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary'],\n",
                        "        num_rows: 14732\n",
                        "    })\n",
                        "    test: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary'],\n",
                        "        num_rows: 819\n",
                        "    })\n",
                        "    validation: Dataset({\n",
                        "        features: ['id', 'dialogue', 'summary'],\n",
                        "        num_rows: 818\n",
                        "    })\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "from datasets import load_dataset\n",
                "\n",
                "## list of dataset for summarization. Choose one of them for your task\n",
                "# https://paperswithcode.com/dataset/cnn-daily-mail-1\n",
                "# data = load_dataset(\"knkarthick/dialogsum\") ##Dialogue Summarization Dataset\n",
                "# data = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n",
                "# data = load_dataset(\"GEM/wiki_lingua\")\n",
                "\n",
                "!pip install -q py7zr\n",
                "data = load_dataset(\"samsum\")\n",
                "\n",
                "print(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "CO7MlU94BUAJ",
                "outputId": "0f8ceace-4e1a-4345-a2ac-cbddcfdca7aa"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Maximum number of tokens in dialogue: 803\n",
                        "Maximum number of tokens in Summary: 64\n"
                    ]
                }
            ],
            "source": [
                "# Using list comprehension to count words in each dialogue\n",
                "word_counts_dialogue = [len(dialogue.split()) for dialogue in data[\"train\"][\"dialogue\"]]\n",
                "# Get the maximum number of words\n",
                "max_words_dialogue = max(word_counts_dialogue)\n",
                "print(f\"Maximum number of tokens in dialogue: {max_words_dialogue}\")\n",
                "\n",
                "word_counts_summary = [len(summary.split()) for summary in data[\"train\"][\"summary\"]]\n",
                "max_words_summary = max(word_counts_summary)\n",
                "print(f\"Maximum number of tokens in Summary: {max_words_summary}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 191
                },
                "id": "7zDbz5CMlzjV",
                "outputId": "0270f526-b893-4b54-c754-ad37b8727269"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpratik_ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "Tracking run with wandb version 0.19.0"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Run data is saved locally in <code>/content/wandb/run-20241211_095702-oou580sp</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Syncing run <strong><a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3/runs/oou580sp' target=\"_blank\">lyric-snow-1</a></strong> to <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View project at <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run at <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3/runs/oou580sp' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3/runs/oou580sp</a>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "# integrate Weights & Biases (W&B) with training process for tracking, monitoring, and collaboration\n",
                "import os\n",
                "import wandb\n",
                "\n",
                "wandb.login(key=os.environ[\"WB_KEY\"])\n",
                "run = wandb.init(\n",
                "    project=\"gemma-2-27b-it_ft_summarizer_v3\",\n",
                "    job_type=\"training\",\n",
                "    anonymous=\"allow\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "6RgbP27YhzNK"
            },
            "outputs": [],
            "source": [
                "# preprcessing before passing input\n",
                "def create_prompt(example):\n",
                "    prefix_text = \"Summarize dialogue in one sentence:\"\n",
                "    text = f\"\"\"<start_of_turn>user\\n {prefix_text} {example['dialogue']} <end_of_turn>\\n<start_of_turn>model {example['summary']} <end_of_turn>\"\"\"\n",
                "    return [text]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "9RnurOEgHkdD"
            },
            "source": [
                "# **LORA Finetuning**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "psOtoPXIIGnq"
            },
            "source": [
                "**Load pre-trained model**\n",
                "\n",
                "\n",
                " **BitsAndBytes**, a library designed to facilitate efficient loading and inference of LLMs with reduced precision. This is particularly useful for deploying models on hardware with limited memory resources.\n",
                "\n",
                "1. `use_4bit`\n",
                "\n",
                "- **Definition**: This parameter activates the loading of base models in 4-bit precision.\n",
                "- **Purpose**: Using 4-bit precision significantly reduces the memory footprint of the model, allowing larger models to fit into GPU memory. This is especially beneficial for inference tasks where high throughput is required but full precision is not necessary.\n",
                "- **Implications**: When set to `True`, the model weights are quantized to 4 bits, which can lead to a trade-off between model performance (accuracy) and resource efficiency. This setting is particularly useful when deploying large models in production environments where memory constraints are a concern.\n",
                "\n",
                "2. `bnb_4bit_compute_dtype`\n",
                "\n",
                "- **Definition**: This parameter specifies the data type used for computations involving 4-bit models.\n",
                "- **Options**: The common options include:\n",
                "  - **`bfloat16`**: Half-precision floating-point format, which uses 16 bits per value.\n",
                "  - **`float32`**: Single-precision floating-point format, using 32 bits per value.\n",
                "- **Purpose**: By setting this parameter to `float16`, you enable faster computations while still maintaining a reasonable level of numerical stability. Using `float16` can improve performance on compatible hardware (like NVIDIA GPUs with Tensor Cores) by allowing for faster matrix operations and reduced memory bandwidth usage.\n",
                "- **Implications**: The choice of compute dtype can affect both the speed and accuracy of the model's predictions. While `float16` can speed up computations, it may also introduce some numerical inaccuracies compared to using `float32`.\n",
                "\n",
                "3. `bnb_4bit_quant_type`\n",
                "\n",
                "- **Definition**: This parameter specifies the type of quantization used for the 4-bit model weights.\n",
                "- **Options**:\n",
                "  - **`fp4`**: A specific quantization format that uses floating-point representations optimized for low precision.\n",
                "  - **`nf4`**: Another format that stands for \"Narrow Float 4,\" which is designed to provide better accuracy at lower bit widths by utilizing a narrower representation.\n",
                "- **Purpose**: The choice of quantization type can significantly impact both the model's performance and its memory efficiency. Different quantization schemes can yield varying levels of accuracy when using low-bit representations.\n",
                "- **Implications**: Selecting `nf4` may provide better performance in terms of maintaining model accuracy compared to `fp4`, depending on the specific characteristics of the model and task.\n",
                "\n",
                "4. `use_nested_quant`\n",
                "\n",
                "- **Definition**: This parameter activates nested quantization for 4-bit base models, also known as double quantization.\n",
                "- **Purpose**: Nested quantization involves applying quantization techniques multiple times (e.g., first quantizing weights down to a lower precision and then further quantizing those results). This can help achieve even lower memory usage while attempting to maintain performance.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "referenced_widgets": [
                        "71fd525cc99f42beadc057bba2740972",
                        "4674a581cdde42f8b58bc6ba4a472e5a",
                        "50b7e8256db64d8da28cca3efb6e0a6e",
                        "194c9cf838f5495a8fbd2ae7e91e95e5",
                        "0391f181bf294c90bebcd286d0902f5f",
                        "4c58f4dea7e64561a693b68aa2377f98",
                        "8903247a872e42fb804b5ba2bb6edc35",
                        "0f8edf5ae97f407684dcb7125c00d4b8",
                        "40d04cd2e5954ceabfba69192cb02fe9",
                        "3ceb56efa56e4066a7b08c37a9f9ae7e",
                        "767185f96f77466aa6f212eccc54afd7",
                        "780174c25ceb4b34ae271d5ba0834911",
                        "f4977703224e43fa95dbb0d0943fb17b",
                        "1888ac01068e4dfe92ebbf119217f2f1",
                        "1b08f5e12aa54bb9b22f180ea7f2ade3",
                        "c948714ea1554928840b00571308c6ce",
                        "3ebfcebb31de4c7799934cbef88ccfe8",
                        "9e53ea7b09584aa5a91e09b1c3dbe976",
                        "5710f51030674e62ad151286adab32c2",
                        "5f88083aca39438db80cd256985e6793",
                        "1dbefe13fc744415922b021f59ff09a2",
                        "bb1a0f6aa6a3493db372b4134006b8d6",
                        "8dba08b330894010a31914887e06075b",
                        "c00e068d774445cf95108b28738cf0ce",
                        "9c226531e7764d7c9060732747cd67c1",
                        "c06a172376f447c78908b925d50c5303",
                        "5a99d95ee27c4ca49f085c8aa1e8686b",
                        "4c0fec8ebb76468480050a162a3722ad",
                        "81e7805274d7437c9b0a8bb873ad50f4",
                        "c4ff398825c542db81c7859b0d97bc2d",
                        "f1c170f25e364efca7cc04c46c42ca40",
                        "528aec7cbe6941c8ba71b845e289d060",
                        "e11b0de50d41427db8ec825f2c78cf56",
                        "7014d51b13d3455986ed37c3990b06ef",
                        "e036ce410e1443e8b7dda7696d24366b",
                        "46e67ce3f5e14df9a4ecadc7167fc5e7",
                        "0c6f8b296cf3485e976de36c54e6da22",
                        "2f4b87b36404488d8958c7ac346caf5d",
                        "1e286c77454a480fb6f3444f9be1a79d",
                        "6fae6fab7b484b1d9ffe7df7d3946f16",
                        "a3d3d5873166412eb5295df56e533ed1",
                        "be27cce0719a45d4b53a8976fe04d456",
                        "98ebdbaeab774089a7e532dfaba34df5",
                        "1af0f3775e67456b869b39bb728c4eb1",
                        "a08a7fb4817e406c87ada64dfbd97f3a",
                        "b8ba4406f9424fa5b7a6d895b2218f71",
                        "ea12485a72714225848715124876ffcb",
                        "f2a7ee3854dc447ba5fc3708a6eb0ce4",
                        "d261226c6be74c8c9b603de2c4676dbc",
                        "2ce4dbfb1c8242c286f8e8f20d063689",
                        "7b04da8c56894fa29a2fd4045ba5731b",
                        "10b5344564434c67971676efc81ddcf0",
                        "aafc3fdfca3f4b6c927328d32a5498d2",
                        "2b48cb01a2f844518e3008aacfe06f17",
                        "30e00f10027b45f38ede04f3426d966a",
                        "5cf1b01694af4716b2459e486bc07aec",
                        "700758107edd4c7eba471fb1bd3d48ca",
                        "31375a60387d474ba1e797538417c8b8",
                        "3d31ff934ace4c70bc26044b73dce7b8",
                        "2538f3f84f4247759d264a413432a9c8",
                        "6d2076f3fa894e8ebc3455ba7ebb52e4",
                        "c717df6069f440fd85fe20ecf08164e8",
                        "10fdeb2e72994fed8c9727f02f561e65",
                        "45cdca7913524daeb31cfc597fab0897",
                        "c14557e97636401cafc551ee7274eb48",
                        "3ed573bf5c8249f4ab2600f3a4fc342b",
                        "e4a46731992c490bab82d55fcb158fea",
                        "059711f1c192447f8a38283fac1a548c",
                        "e1eb06326f7a4557ab5e591240736a29",
                        "bd2405cdc08248499bfe4d013ec746d2",
                        "757bd12de03241d889de04c9fa57bb15",
                        "4b22e1b0df19442a95fd0767f62762ff",
                        "f3c1805450cd4538a024162717437036",
                        "45cd400ae8984c4dbf10e6af871a32f3",
                        "4fef44ac8834409398716e2f189756c5",
                        "d84f9baae249435b82497452548a4e3c",
                        "73ca7b64d28d40d7aab77bf67da33819",
                        "d385ac488d984be780c032f85346549f",
                        "54459b4af9b048cdaa441d5163554cb0",
                        "9c1b3171fae54389bcd56efd75859ed2",
                        "4ad880344ff145fc8d81a2f6a2355764",
                        "2b6c2c4535f14944bf780cf71641f322",
                        "af15bba2513046bda248360744553340",
                        "025734d072354881b98a5ef0fcc85e52",
                        "247400d74b8e4117b361dfd5a7c2b599",
                        "768477ae466c4534941b43524e810eb0",
                        "681f56ab41f548e2b019d31e1b6bff22",
                        "e6a538ea81884870b3355bcd62fa658d",
                        "9fe2914ef88745ffa1a3a01a8ba86653",
                        "416143ee93d9494692ddbf12826f3967",
                        "8a7fdc819a7642bbb097486f40d9ebcf",
                        "d81517a515374ed184515132482d429c",
                        "51332b0d32434dc79494211e1c650461",
                        "b0c2dfa2b45d479fb133a559946c7ee3",
                        "980983aabb3e477082f71157bd5ba5f5",
                        "f580d418afc046c094d15e9451405db7",
                        "bb8ebefa0d994780a3eb74c8c6b62207",
                        "ebd4a0a03a254eaca0b58b81345a1488",
                        "14e8051161044ac4a0a74f9533757af5",
                        "b7a4a96693b6445a8dbdd3768e6a0c1f",
                        "c5007f3073744fcbb8949b8807759ee9",
                        "073c9da715ac42d994dce124d5d3902d",
                        "ffafc993bde94d14a110452be38d8550",
                        "9a9d27855c164e17b9ebd88b989486c8",
                        "0668c253246b434db778e96b35d0f80e",
                        "d8f7d071abe34806a3232a0e535592ab",
                        "ac3664c94faf495ab5feed9b09d83d48",
                        "522292b6d7514089a1f9bcaa8c42f8a5",
                        "b93e4269361544e08aeb98707ad9550c",
                        "5909341cb10f43ae9bed10c2e0022c68",
                        "88cd7a2e68c04e0bbe188ba15f3b2966",
                        "594855272465485bbc94ed50d5115c13",
                        "05449a683f644d679bd6ad63198c99ba",
                        "6887741c01004c6592b0ee3244c5448a",
                        "03e0f004002f4475b69f6ad1fd349b12",
                        "fe7fa05b191b45b2b146f86b173797ae",
                        "41d1b906e1e741d38051f46ccfe9bc5f",
                        "7e6d0519d88040a384ee4a6463baba60",
                        "32d1db52f7fd45c4a80f46c582da0d8a",
                        "cc12e9ce118f4bccb24ee20bd42576fc",
                        "36541254306943718ad62fa251be947b",
                        "510517fc1a8440f391c12e85d26adb93",
                        "8ef7acd78b01491893b1023c5f932362",
                        "7dcf791537684778ba829d491cda05da",
                        "801a530a31594930b5402c51a8f09f81",
                        "c135fe8c66c94bb4a0040a05137e338c",
                        "ca74cbe2430845578980e94f67e12ad9",
                        "e0e393b7279a446ca4b6d2e758d603fc",
                        "3aa239895dd64b4b8088a90f3c9df929",
                        "b6ed479f48414725a85887255ed2dc8a",
                        "b2786ea6b8e54e7eaed8f8d354bee7e2",
                        "eff6f013a4de4f6e9c8b72034d842f92",
                        "8efc0a56c87349ac831abf764911655d",
                        "19a03c1a2ea24442bd8b2043f9e9d460",
                        "5297622bbaea43bcae359facf8cba8eb",
                        "a5e5bec09683483faa496a715dbcbef4",
                        "9b978833e6c545d79c2e2d0981e73ad7",
                        "4f56a5b039114a269f3306385cae577f",
                        "4020110bbbce4af1b23c4762fee1273a",
                        "f05dd71b9f2b4d9488a3995229fe3f9a",
                        "75e751d274fe4324870b626b9a6cac46",
                        "68d329c2f60c4ee4bd7c00a0456000d8",
                        "d3013167065c46b7991626184b5cb202",
                        "ca239beeed1741ba8985093fc1e30cda",
                        "598d5fd6727341a5b259975a63704a0c",
                        "1bce8877a4a7479abbcfa19ad18e90bd",
                        "b038a13a83174268a49da377cb9d8826",
                        "337f2cf3a984415eb09db59ad1178368",
                        "6b8d31d845914b41835cf48d910122af",
                        "25ebb2fddd484f6088136a6568daf596",
                        "cbea458057fe41498321791e7cf83f48",
                        "0f68d902d41f479cac98c122ab5549ac",
                        "7287d0179c2842a2b93e008e2949122b",
                        "6ea593b3c48a4f5a94e208492e0ced45",
                        "6a888466cfe94b39b0b985df6653276f",
                        "9fa525faf5704c4dadcfd9c9c88cd325",
                        "64de8a88593948f3979e1a0146f7e9c3",
                        "e81f3414bba548049b9f8a8121924836",
                        "d2177cfc36a64ba5b1c658423f58cf7d",
                        "55368e681f8341c899d09abb7e152956",
                        "2b4f9717c285452cb33479f6f95bdec8",
                        "acdbcbb88fa148a8b0236aa94491508f",
                        "cf69054a10e646089ee53982e7a5e358",
                        "68392a95d56346bbb38ef68e735ade0a",
                        "e254e32c7f0d47259d10a21b1e4821ed",
                        "cf9d961423204fdab8f03e6d8c906a86",
                        "571a8fcf58214f9c9e9803c451b44193",
                        "5a08059194a9401faf28b404b288245c",
                        "6b2a0fe470b449678766e92feb1fa8f9",
                        "f0bb7e9daaf741e8815c1ddb7dd01496",
                        "6d6784759a4a4536a7e76b89f210241e",
                        "ca54382c455b4f1e8287496b5b0cddf3",
                        "d478a8aeac4e4a5483e402883c1e6a9b",
                        "55eb58a83dc045298fd401b744858309",
                        "265aaf86cbcb4e86ba94a4b038db6268",
                        "fa36ac4277124cd9bbaa9de18293fca1",
                        "c09f26521441453caefb485cc13c57de",
                        "09aa24cd6ff4443982158536795d2baf",
                        "b2dce2ea478b466eb4cde37016661136",
                        "318c71060212456ca8d9f1f94ffaec01",
                        "41103fdaaedc418a852d87a00d333bf3",
                        "0a7582edbc6e447ab080502a4a538c87",
                        "03e699af693747dd89bfd8dbe9970251",
                        "315558a530464f129d9255aad27beff9",
                        "3ceefb764d614ccc976df7c6053f557f",
                        "596e3ed6d0f746848c30fe3c10d5d068",
                        "a65524056fd14820ae841bce8a86fc2a"
                    ]
                },
                "id": "5lKaicteiydl",
                "outputId": "b2ca9112-3b63-45f2-9b4f-024e82542b9c"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "71fd525cc99f42beadc057bba2740972",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "config.json:   0%|          | 0.00/893 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "780174c25ceb4b34ae271d5ba0834911",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors.index.json:   0%|          | 0.00/42.8k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8dba08b330894010a31914887e06075b",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading shards:   0%|          | 0/12 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7014d51b13d3455986ed37c3990b06ef",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00012.safetensors:   0%|          | 0.00/4.74G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a08a7fb4817e406c87ada64dfbd97f3a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "5cf1b01694af4716b2459e486bc07aec",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e4a46731992c490bab82d55fcb158fea",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "d385ac488d984be780c032f85346549f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00005-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "9fe2914ef88745ffa1a3a01a8ba86653",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00006-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b7a4a96693b6445a8dbdd3768e6a0c1f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00007-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "88cd7a2e68c04e0bbe188ba15f3b2966",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00008-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "510517fc1a8440f391c12e85d26adb93",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00009-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8efc0a56c87349ac831abf764911655d",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00010-of-00012.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ca239beeed1741ba8985093fc1e30cda",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00011-of-00012.safetensors:   0%|          | 0.00/4.87G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "6a888466cfe94b39b0b985df6653276f",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00012-of-00012.safetensors:   0%|          | 0.00/680M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "cf9d961423204fdab8f03e6d8c906a86",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c09f26521441453caefb485cc13c57de",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gemma2ForCausalLM(\n",
                        "  (model): Gemma2Model(\n",
                        "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
                        "    (layers): ModuleList(\n",
                        "      (0-45): 46 x Gemma2DecoderLayer(\n",
                        "        (self_attn): Gemma2Attention(\n",
                        "          (q_proj): Linear4bit(in_features=4608, out_features=4096, bias=False)\n",
                        "          (k_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
                        "          (v_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
                        "          (o_proj): Linear4bit(in_features=4096, out_features=4608, bias=False)\n",
                        "          (rotary_emb): Gemma2RotaryEmbedding()\n",
                        "        )\n",
                        "        (mlp): Gemma2MLP(\n",
                        "          (gate_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
                        "          (up_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
                        "          (down_proj): Linear4bit(in_features=36864, out_features=4608, bias=False)\n",
                        "          (act_fn): PytorchGELUTanh()\n",
                        "        )\n",
                        "        (input_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (post_attention_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (pre_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (post_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "      )\n",
                        "    )\n",
                        "    (norm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "  )\n",
                        "  (lm_head): Linear(in_features=4608, out_features=256000, bias=False)\n",
                        ")\n",
                        "\n",
                        "\n",
                        "--->Execution Time: 25.599624347686767 minutes\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "\n",
                "# #Load base/pretrained model for training\n",
                "\n",
                "# Clear GPU cache\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "# Load model for training\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                "    # Enable CPU offloading for specific layers\n",
                "    llm_int8_enable_fp32_cpu_offload=False,\n",
                ")\n",
                "\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    torch_dtype=torch.bfloat16,  # <-- important to set bfloat16\n",
                "    device_map=\"auto\",  # Let Transformers automatically decide device placement\n",
                ")\n",
                "\n",
                "print(model)\n",
                "\n",
                "end_time = time.time()\n",
                "print(\"\\n\\n--->Execution Time:\", (end_time - start_time) / 60, \"minutes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Xda8ZyX9Dtjw",
                "outputId": "65b908d1-43f8-4edb-a461-ec22caeb0b92"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
                        "  warnings.warn(\n",
                        "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "user\n",
                        "Summarize dialogue one sentence:\n",
                        "Amanda: I baked  cookies. Do you want some?\n",
                        "Jerry: Sure!\n",
                        "Amanda: I'll bring you tomorrow :-)\n",
                        "model\n",
                        "Amanda offers Jerry some cookies and promises to bring them to him the next day. \n",
                        "\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "text = \"\"\"user\n",
                "Summarize dialogue one sentence:\n",
                "Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\n",
                "model\n",
                "\"\"\"\n",
                "device = \"cuda:0\"\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"left\"\n",
                "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "\n",
                "outputs = model.generate(**inputs, max_new_tokens=50, temperature=0.1)\n",
                "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "apV3e_rTlPjR"
            },
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "HeP4765LIQd8"
            },
            "source": [
                "**Load Dataset ( train and validation )**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Ne0RAbkvHj3h",
                "outputId": "c61f776b-e318-4b3d-dc8f-5e5ec3a07b5f"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset({\n",
                        "    features: ['id', 'dialogue', 'summary'],\n",
                        "    num_rows: 14732\n",
                        "})\n",
                        "Dataset({\n",
                        "    features: ['id', 'dialogue', 'summary'],\n",
                        "    num_rows: 818\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "from datasets import DatasetDict\n",
                "\n",
                "# TRAIN_DATA_RECORD_SIZE = 1400  # <-----\n",
                "# VAL_DATA_RECORD_SIZE = 300\n",
                "\n",
                "dataset_dict = DatasetDict(data)\n",
                "# Extract the first 100 rows from the training dataset\n",
                "# training_dataset = dataset_dict[\"train\"].select(range(TRAIN_DATA_RECORD_SIZE))\n",
                "training_dataset = dataset_dict[\"train\"]\n",
                "\n",
                "# Extract the first 100 rows from the training dataset\n",
                "# val_dataset = dataset_dict[\"validation\"].select(range(VAL_DATA_RECORD_SIZE))\n",
                "val_dataset = dataset_dict[\"validation\"]\n",
                "\n",
                "print(training_dataset)\n",
                "print(val_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "ZLoOuMLuPMnN",
                "outputId": "a395dc3b-e565-42a8-c7ec-abf748666e00"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'id': '13818513',\n",
                            " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
                            " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
                        ]
                    },
                    "execution_count": 12,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "training_dataset[0]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "_Awy_qiyI4_4"
            },
            "source": [
                "**Set best LORA hyper-parameters**\n",
                "\n",
                "```Target Modules```\n",
                "\n",
                "1. `q_proj` (Query Projection):\n",
                "   - **Definition**: This module is responsible for projecting the input embeddings into the query space.\n",
                "   - **Functionality**: In the attention mechanism, the query vectors are derived from the input embeddings to determine how much focus should be placed on different parts of the input sequence.\n",
                "   - **Role in Attention**: The query vectors are compared against key vectors to compute attention scores, which dictate how much attention each token should pay to others.\n",
                "\n",
                "2. `o_proj` (Output Projection):\n",
                "   - **Definition**: This module is used to project the output of the attention mechanism back into the original embedding space.\n",
                "   - **Functionality**: After calculating attention scores and aggregating values, the resulting output needs to be transformed back to match the dimensionality of the input embeddings for further processing.\n",
                "   - **Role in Attention**: It ensures that the output from the attention layer can be fed into subsequent layers of the model, maintaining consistency in dimensions.\n",
                "\n",
                "3. `k_proj` (Key Projection):\n",
                "   - **Definition**: This module projects input embeddings into the key space.\n",
                "   - **Functionality**: Similar to query projection, key projection transforms input embeddings into key vectors that are used in conjunction with query vectors during the attention calculation.\n",
                "   - **Role in Attention**: The keys are compared with queries to generate attention scores, which determine how relevant each token is concerning others.\n",
                "\n",
                "4. `v_proj` (Value Projection):\n",
                "   - **Definition**: This module projects input embeddings into the value space.\n",
                "   - **Functionality**: Value vectors represent the actual content that will be aggregated based on attention scores.\n",
                "   - **Role in Attention**: After computing attention weights from queries and keys, these weights are applied to value vectors to produce a weighted sum that forms the output of the attention mechanism.\n",
                "\n",
                "5. `gate_proj` (Gate Projection):\n",
                "   - **Definition**: This module is part of a gating mechanism often used in more complex architectures or specific models like transformers with additional control over information flow.\n",
                "   - **Functionality**: Gates can modulate how much information passes through certain layers or components based on learned parameters.\n",
                "   - **Role in Attention/Modeling**: It helps manage which parts of information are retained or discarded during processing, enhancing model flexibility and performance.\n",
                "\n",
                "6. `up_proj` (Upward Projection):\n",
                "   - **Definition**: This module typically refers to a projection that increases dimensionality or transforms data into a higher-dimensional space.\n",
                "   - **Functionality**: In certain architectures, upward projections can be used to expand feature representations before passing them through non-linear transformations or additional layers.\n",
                "   - **Role in Model Structure**: It can help capture more complex relationships by allowing for richer representations at certain stages of processing.\n",
                "\n",
                "7. `down_proj` (Downward Projection):\n",
                "   - **Definition**: This module reduces dimensionality or transforms data into a lower-dimensional space.\n",
                "   - **Functionality**: Downward projections can be used to condense information after processing through multiple layers or operations, making it more manageable for subsequent computations.\n",
                "   - **Role in Model Structure**: It helps streamline data flow and reduce computational overhead while retaining essential features.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "id": "NeAOVU3CCmOY"
            },
            "outputs": [],
            "source": [
                "################################################################################\n",
                "# LoRA parameters\n",
                "################################################################################\n",
                "best_lora_dropout = 0.3\n",
                "best_lora_r = 2\n",
                "best_lora_alpha = 4\n",
                "best_target_modules = [\n",
                "    \"q_proj\",\n",
                "    \"o_proj\",\n",
                "    \"k_proj\",\n",
                "    \"v_proj\",\n",
                "    \"gate_proj\",\n",
                "    \"up_proj\",\n",
                "    \"down_proj\",\n",
                "]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "F4ltgmsVJEwQ"
            },
            "source": [
                "**Method to log CPU/ memory usage matrices during training**\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "id": "WYITQtdP4WKX"
            },
            "outputs": [],
            "source": [
                "resource_usage_training_df = pd.DataFrame(columns=[\"cpu_usage\", \"memory_usage\"])\n",
                "\n",
                "\n",
                "def log_resource_usage(stage):\n",
                "    # CPU and memory usage\n",
                "    # stage=trial.number\n",
                "    cpu_usage = psutil.cpu_percent(interval=1)\n",
                "    memory_usage = psutil.virtual_memory().percent\n",
                "    print(f\"CPU Usage: {cpu_usage}%\")\n",
                "    print(f\"Memory Usage: {memory_usage}%\")\n",
                "\n",
                "    # GPU usage\n",
                "    gpus = GPUtil.getGPUs()\n",
                "    for gpu in gpus:\n",
                "        gpu_memory_used = gpu.memoryUsed\n",
                "        gpu_memory_total = gpu.memoryTotal\n",
                "        gpu_utilization = gpu.load\n",
                "        print(\n",
                "            f\"GPU {gpu.id} - Memory Usage: {gpu.memoryUsed}/{gpu.memoryTotal} MB - Utilization: {gpu.load * 100}%\"\n",
                "        )\n",
                "\n",
                "    # Initialize a DataFrame to store resource usage metrics\n",
                "    # Append the metrics to the DataFrame\n",
                "\n",
                "    # Create a dictionary of the metrics\n",
                "    metrics = {\n",
                "        \"stage\": stage,\n",
                "        \"cpu_usage\": cpu_usage,\n",
                "        \"memory_usage\": memory_usage,\n",
                "        \"gpu_memory_used\": gpu_memory_used,\n",
                "        \"gpu_memory_total\": gpu_memory_total,\n",
                "        \"gpu_utilization\": gpu_utilization * 100,  # Convert to percentage\n",
                "    }\n",
                "    # Append the metrics to the DataFrame\n",
                "    global resource_usage_training_df\n",
                "    resource_usage_training_df = pd.concat(\n",
                "        [resource_usage_training_df, pd.DataFrame([metrics])], ignore_index=True\n",
                "    )\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "fXh40YH6JUUH"
            },
            "source": [
                "**LORA config and training Arguments**\n",
                "\n",
                "\n",
                "`TrainingArguments Parameter`\n",
                "\n",
                "1. **`per_device_train_batch_size`**:\n",
                "   - **Definition**: This parameter sets the batch size for training on each device (e.g., GPU).\n",
                "   - **Details**: A batch size of `1` means that each training step will process one sample at a time. Smaller batch sizes can lead to more frequent updates but may result in noisier gradients and longer training times.\n",
                "\n",
                "2. **`per_device_eval_batch_size`**:\n",
                "   - **Definition**: This parameter sets the batch size for evaluation on each device.\n",
                "   - **Details**: Similar to the training batch size, a batch size of `1` for evaluation means that one sample will be evaluated at a time. This can be useful for memory-constrained environments or when evaluating models on large datasets.\n",
                "\n",
                "3. **`gradient_accumulation_steps`**:\n",
                "   - **Definition**: This parameter specifies how many steps to accumulate gradients before performing a backward/update pass.\n",
                "   - **Details**: Setting this to `2` means that gradients will be accumulated over 2 steps before updating the model weights. This effectively simulates a larger batch size without increasing memory usage, which can be beneficial when working with limited GPU memory.\n",
                "\n",
                "4. **`num_train_epochs`**:\n",
                "   - **Definition**: This parameter indicates the total number of epochs for training.\n",
                "   - **Details**: An epoch is one complete pass through the entire training dataset. The variable `num_epochs` should be defined elsewhere in your code, determining how many times the model will see the entire dataset during training.\n",
                "\n",
                "5. **`warmup_steps`**:\n",
                "   - **Definition**: This parameter specifies the number of steps for linear learning rate warmup.\n",
                "   - **Details**: During warmup, the learning rate increases linearly from `0` to the initial learning rate over the specified number of steps. This helps stabilize training in the early phases and can prevent large gradient updates that might destabilize learning.\n",
                "\n",
                "6. **`evaluation_strategy`**:\n",
                "   - **Definition**: This parameter determines when to evaluate the model during training.\n",
                "   - **Details**: Setting this to `\"steps\"` means that evaluation will occur at regular intervals defined by `eval_steps`.\n",
                "\n",
                "7. **`eval_steps`**:\n",
                "   - **Definition**: This parameter specifies how often to evaluate the model during training.\n",
                "   - **Details**: The value `0.1` typically indicates that evaluation will occur every 10% of the total number of training steps.\n",
                "\n",
                "8. **`learning_rate`**:\n",
                "   - **Definition**: This parameter sets the initial learning rate for the optimizer.\n",
                "   - **Details**: A learning rate of `1e-4` (0.0001) is balancing between convergence speed and stability.\n",
                "\n",
                "9. **`weight_decay`**:\n",
                "   - **Definition**: This parameter applies weight decay (L2 regularization) to prevent overfitting by penalizing large weights.\n",
                "   - **Details**: A weight decay value of `1e-2` (0.01) helps regularize the model, encouraging smaller weights and potentially improving generalization.\n",
                "\n",
                "10. **`fp16`**:\n",
                "    - **Definition**: This parameter enables mixed precision training using 16-bit floating-point (FP16) format.\n",
                "    - **Details**: Setting this to `False` means that FP16 training is disabled, and full precision (FP32) will be used instead.\n",
                "\n",
                "11. **`bf16`**:\n",
                "    - **Definition**: This parameter enables bfloat16 precision, which is particularly useful for training on TPUs or specific GPUs.\n",
                "    - **Details**: Setting this to `True` allows using bfloat16, which can provide similar benefits as FP16 while maintaining a wider dynamic range, reducing issues with underflow.\n",
                "\n",
                "12. **`logging_steps`**:\n",
                "    - **Definition**: This parameter specifies how often to log training metrics.\n",
                "    - **Details**: A value of `1` means that metrics will be logged after every step, which can provide detailed insights into model performance during training.\n",
                "\n",
                "13. **`output_dir`**:\n",
                "    - **Definition**: This parameter specifies where to save model checkpoints and logs.\n",
                "    - **Details**: The directory `\"train_outputs\"` will contain all saved models and logs during training.\n",
                "\n",
                "14. **`optim`**:\n",
                "    - **Definition**: This parameter specifies which optimizer to use during training.\n",
                "    - **Details**: Setting this to `\"paged_adamw_8bit\"` indicates that a specific variant of AdamW optimized for 8-bit precision will be used, which can help reduce memory usage while maintaining efficiency.\n",
                "\n",
                "15. **`report_to`**:\n",
                "    - **Definition**: This parameter determines where to report metrics during training.\n",
                "    - **Details**: Setting this to `\"wandb\"` indicates that metrics will be reported to Weights & Biases (WandB). other options is `\"tensorboard\"`.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "c_7ZQWEFDJcM",
                "outputId": "214786d4-9749-4446-d6b0-5633ee97f898"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "# Define LoRA configuration with the best hyperparameters\n",
                "lora_config = LoraConfig(\n",
                "    r=best_lora_r,\n",
                "    lora_alpha=best_lora_alpha,\n",
                "    lora_dropout=best_lora_dropout,\n",
                "    target_modules=best_target_modules,\n",
                "    task_type=\"CAUSAL_LM\",\n",
                ")\n",
                "\n",
                "\n",
                "# NUM_OF_EPOCHS = 20  # <----\n",
                "training_arguments = transformers.TrainingArguments(\n",
                "    per_device_train_batch_size=1,\n",
                "    per_device_eval_batch_size=1,\n",
                "    gradient_accumulation_steps=2,\n",
                "    gradient_checkpointing=True,\n",
                "    # num_train_epochs=NUM_OF_EPOCHS,\n",
                "    warmup_steps=3,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    eval_steps=0.1,\n",
                "    max_steps=75,\n",
                "    learning_rate=1e-4,\n",
                "    weight_decay=1e-2,\n",
                "    fp16=False,\n",
                "    bf16=True,\n",
                "    logging_steps=1,\n",
                "    output_dir=\"train_outputs\",\n",
                "    optim=\"paged_adamw_8bit\",\n",
                "    report_to=\"wandb\",\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "X5DtYxbHI6S0"
            },
            "source": [
                "**Model training**\n",
                "\n",
                "The `Accelerator` is used to facilitate distributed training and mixed precision training. It simplifies the process of scaling up your model training across multiple GPUs or even multiple nodes, and it can also help with optimizing memory usage and computational efficiency.\n",
                "\n",
                "Benefits :\n",
                "1. Distributed Training:\n",
                "   - Benefit: Allows the training process to be distributed across multiple GPUs or nodes, which can significantly speed up training times.\n",
                "   - Example: If you have multiple GPUs, `Accelerator` will automatically distribute the model and data across these GPUs, enabling parallel processing. Accelerator manages communication between devices, ensuring that gradients are synchronized correctly.\n",
                "\n",
                "2. Mixed Precision Training:\n",
                "   - Benefit: Reduces memory usage and can speed up training by using lower precision (e.g., `bfloat16`).\n",
                "   - Example: By using mixed precision, you can fit larger models or larger batch sizes into GPU memory, which can improve training efficiency.\n",
                "\n",
                "3. Simplified Device Management:\n",
                "   - Benefit: Automatically handles the placement of tensors on the correct devices, reducing the complexity of managing device-specific code.\n",
                "   - Example: You don't need to manually move tensors to the GPU or handle device-specific operations; `Accelerator` takes care of it.\n",
                "\n",
                "By using `Accelerator`, you can achieve faster training times, better memory utilization, and easier scaling of your model training process.  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rPeXELCPC9pA",
                "outputId": "a53681f1-259f-4f3c-a984-4371226b5b01"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "from transformers import AdamW\n",
                "from accelerate import Accelerator\n",
                "\n",
                "\n",
                "# Initialize the Accelerator\n",
                "accelerator = Accelerator()\n",
                "\n",
                "# Ensure pad token is set\n",
                "tokenizer.pad_token = tokenizer.eos_token\n",
                "tokenizer.padding_side = \"left\"  # as it is a decoder-only model, it is recommended to set padding_side to \"left\".\n",
                "\n",
                "# Initialize the optimizer\n",
                "optimizer = AdamW(model.parameters(), lr=training_arguments.learning_rate)\n",
                "\n",
                "# Prepare the model, tokenizer, datasets, and optimizer with the Accelerator\n",
                "model, optimizer, training_dataset, val_dataset = accelerator.prepare(\n",
                "    model, optimizer, training_dataset, val_dataset\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 979,
                    "referenced_widgets": [
                        "91c6121b9a734d6eac1be03f7bd77353",
                        "f166ee91d4a5455dbab2a9d7b487a2b4",
                        "ca6cb7cde9634f1cb9cca7a4916e70c6",
                        "ae843e0ddd074a5cae153d5da2b17b77",
                        "cde530597793419bb28683f864f04a6f",
                        "f2a83ef8dcfd454e8d45da016fd8568a",
                        "ceacea96d27a4b39a1246ee05cffa674",
                        "17ca8ef414db4255a4828f34117a64ac",
                        "f417e0ca831f4da4824441b20ebf7c59",
                        "17207ff091c24988b5f27f8a93758f53",
                        "93e4af6885cd4857a4f388ba6bd6fa74",
                        "89096e7873a347eeb7487fddb1c9cc6c",
                        "65a28c7a76ab4a9da7340d560282b973",
                        "e745d787b38a40129acc65dc9e29af5e",
                        "f62f6d05989947f5a43f494106eae96f",
                        "a7323d72d38449829fdf5dd48d3c081f",
                        "a519ea95d48740a8b3ff76956c756934",
                        "c3d99260629f444592a45d64d53c963f",
                        "96253452a0bb4b25b466263be828680e",
                        "6504d34f7fe8492a855ece92a3602f72",
                        "dc0c378246fa4ed0a140496b0145e490",
                        "1e4593a0e4134b7eb9cd800c33d36f69"
                    ]
                },
                "id": "W2Rx40XbDYts",
                "outputId": "2a00f61d-9445-4dc1-9325-4219c8fc4ead"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
                        "\n",
                        "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
                        "  warnings.warn(message, FutureWarning)\n",
                        "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n",
                        "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
                        "  warnings.warn(\n",
                        "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
                        "  warnings.warn(\n",
                        "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "91c6121b9a734d6eac1be03f7bd77353",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "89096e7873a347eeb7487fddb1c9cc6c",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resource usage before training:\n",
                        "CPU Usage: 1.0%\n",
                        "Memory Usage: 3.8%\n",
                        "GPU 0 - Memory Usage: 21699.0/40960.0 MB - Utilization: 0.0%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<ipython-input-14-cbeaccda37a0>:36: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
                        "  resource_usage_training_df = pd.concat(\n",
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
                        "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [75/75 03:47, Epoch 9/11]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "      <th>Validation Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>2.151500</td>\n",
                            "      <td>2.076029</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>16</td>\n",
                            "      <td>2.333300</td>\n",
                            "      <td>1.959144</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>24</td>\n",
                            "      <td>2.242100</td>\n",
                            "      <td>1.890339</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>32</td>\n",
                            "      <td>1.796700</td>\n",
                            "      <td>1.847928</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>2.033600</td>\n",
                            "      <td>1.834626</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>48</td>\n",
                            "      <td>1.447000</td>\n",
                            "      <td>1.836888</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>56</td>\n",
                            "      <td>1.949600</td>\n",
                            "      <td>1.844111</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>64</td>\n",
                            "      <td>1.586100</td>\n",
                            "      <td>1.848862</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>72</td>\n",
                            "      <td>1.853600</td>\n",
                            "      <td>1.854472</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Resource usage after training:\n",
                        "CPU Usage: 10.9%\n",
                        "Memory Usage: 3.9%\n",
                        "GPU 0 - Memory Usage: 21871.0/40960.0 MB - Utilization: 0.0%\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "--->Execution Time: 4.10518221060435 minutes\n"
                    ]
                }
            ],
            "source": [
                "from accelerate import DistributedType\n",
                "import time\n",
                "\n",
                "# Function to log resource usage\n",
                "import psutil\n",
                "import GPUtil\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "# Clear GPU cache\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "\n",
                "SAVE_MODEL = True\n",
                "# Initialize Trainer with the best hyperparameters\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    train_dataset=training_dataset,\n",
                "    eval_dataset=val_dataset,\n",
                "    peft_config=lora_config,\n",
                "    max_seq_length=900,  # max length to input. It is crucial for GPU memory management\n",
                "    dataset_text_field=\"dialogue\",  #  field in the dataset that contains the text data to be used for training and evaluation.\n",
                "    formatting_func=create_prompt,  # preprocessing function before input\n",
                "    processing_class=tokenizer,\n",
                "    args=training_arguments,\n",
                "    packing=False,  # The trainer will attempt to pack multiple sequences into a single batch\n",
                ")\n",
                "\n",
                "# Train the final model\n",
                "model.config.use_cache = False\n",
                "\n",
                "# Log resource usage before training\n",
                "print(\"Resource usage before training:\")\n",
                "log_resource_usage(1)\n",
                "\n",
                "\n",
                "# Add the custom callback to the trainer\n",
                "# trainer.add_callback(LoggingCallback)\n",
                "\n",
                "# Use the Accelerator to manage the training loop\n",
                "trainer.train()\n",
                "\n",
                "# Log resource usage before training\n",
                "print(\"Resource usage after training:\")\n",
                "log_resource_usage(2)\n",
                "\n",
                "\n",
                "# Save the final model\n",
                "# accelerator.wait_for_everyone() method is used to synchronize all processes in a distributed training setup,ensuring that all processes reach the same point before proceeding.\n",
                "# This is crucial for maintaining consistency and coordination across multiple devices (e.g., multiple GPUs or TPUs) during training.\n",
                "accelerator.wait_for_everyone()\n",
                "if accelerator.is_local_main_process:\n",
                "    if SAVE_MODEL:\n",
                "        trainer.model.save_pretrained(new_model)\n",
                "        trainer.tokenizer.save_pretrained(new_model)\n",
                "\n",
                "end_time = time.time()\n",
                "print(\"\\n\\n--->Execution Time:\", (end_time - start_time) / 60, \"minutes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 477
                },
                "id": "TMt2TvMcQjW1",
                "outputId": "5b4f2a61-24aa-4bb3-f63d-ea70713a605b"
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <style>\n",
                            "        .wandb-row {\n",
                            "            display: flex;\n",
                            "            flex-direction: row;\n",
                            "            flex-wrap: wrap;\n",
                            "            justify-content: flex-start;\n",
                            "            width: 100%;\n",
                            "        }\n",
                            "        .wandb-col {\n",
                            "            display: flex;\n",
                            "            flex-direction: column;\n",
                            "            flex-basis: 100%;\n",
                            "            flex: 1;\n",
                            "            padding: 10px;\n",
                            "        }\n",
                            "    </style>\n",
                            "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▁▁▁▁▁▂</td></tr><tr><td>eval/runtime</td><td>█▂▄▁█▂▂▅▇</td></tr><tr><td>eval/samples_per_second</td><td>▅███▁██▅▅</td></tr><tr><td>eval/steps_per_second</td><td>▅███▁██▅▅</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▁▂▂▃▄▅▃▁▂▃▄▁▄▄▄▃▃▃▄▄▅▄▅▆▅▆▃▅▆▇▆▇██▃▇▇▇█▅</td></tr><tr><td>train/learning_rate</td><td>▆████▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>██▇▇▇▇▂▆▃▇▆▅▂▅▅▅▆▁▆▅▅▅▂▅▅▅▅▅▅▅▅▅▄▄▆▁▆▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>1.85447</td></tr><tr><td>eval/runtime</td><td>0.5033</td></tr><tr><td>eval/samples_per_second</td><td>1.987</td></tr><tr><td>eval/steps_per_second</td><td>1.987</td></tr><tr><td>total_flos</td><td>1.98434186062848e+16</td></tr><tr><td>train/epoch</td><td>9.4</td></tr><tr><td>train/global_step</td><td>75</td></tr><tr><td>train/grad_norm</td><td>2.90409</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>3.7336</td></tr><tr><td>train_loss</td><td>3.53057</td></tr><tr><td>train_runtime</td><td>231.3494</td></tr><tr><td>train_samples_per_second</td><td>0.648</td></tr><tr><td>train_steps_per_second</td><td>0.324</td></tr></table><br/></div></div>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            " View run <strong style=\"color:#cdcd00\">lyric-snow-1</strong> at: <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3/runs/oou580sp' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3/runs/oou580sp</a><br/> View project at: <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Find logs at: <code>./wandb/run-20241211_095702-oou580sp/logs</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "wandb.finish()\n",
                "model.config.use_cache = True"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8WsXhPnq_oMG"
            },
            "source": [
                "## Merge finetuned LORA with pre-trained model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {
                "id": "8cUpNUkMlb-G"
            },
            "outputs": [],
            "source": [
                "model_id = \"google/gemma-2-27b-it\"\n",
                "new_model = \"gemma-2-27b-it_ft\"\n",
                "\n",
                "import os\n",
                "\n",
                "os.environ[\"HF_TOKEN\"] = \"HF_KEY\"\n",
                "os.environ[\"WB_KEY\"] = \"WB_KEY\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 34
                },
                "id": "Co89q2WWSKh2",
                "outputId": "7e649f8e-2c09-42bd-9b54-da59d9e6ba61"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
                    ]
                }
            ],
            "source": [
                "# integrate Weights & Biases (W&B) with training process for tracking, monitoring, and collaboration\n",
                "import os\n",
                "import wandb\n",
                "\n",
                "wandb.login(key=os.environ[\"WB_KEY\"])\n",
                "run = wandb.init(\n",
                "    project=\"gemma-2-27b-it_ft_summarizer_v3\",\n",
                "    job_type=\"training\",\n",
                "    anonymous=\"allow\",\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "SQa34KXiLPK0"
            },
            "outputs": [],
            "source": [
                "from peft import LoraConfig\n",
                "from datasets import load_dataset\n",
                "from datasets import load_metric\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "\n",
                "import transformers\n",
                "from rouge_score import rouge_scorer\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 225,
                    "referenced_widgets": [
                        "a100595b128f499a80d22d29aef89920",
                        "f8673a612fd241368c96882866fa80ec",
                        "0cad1128149047da97281130f3bca046",
                        "9814d3105cec4316b26fb923e47cae7d",
                        "2563784fa026414e82e5a9b59e99e2fd",
                        "95490fd5875040198d0b71c991501380",
                        "2e50bd51606d4c84aca0842b563e6a1b",
                        "008129c4612e46e08825475679ed3dd3",
                        "391002856d674b8bbd3370efa21c2de0",
                        "8e0b4246e4da42d48e36114f7a1038e2",
                        "e753da188cbb4cb880a2b96758768218"
                    ]
                },
                "id": "146HnE6PKggG",
                "outputId": "57978ea9-252b-41f4-d538-abf89b34d325"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
                        "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
                        "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
                        "You will be able to reuse this secret in all of your notebooks.\n",
                        "Please note that authentication is recommended but still optional to access public models or datasets.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "a100595b128f499a80d22d29aef89920",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "--->Execution Time: 3.8909881194432576 minutes\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import time\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "# Clear GPU cache\n",
                "torch.cuda.empty_cache()\n",
                "\n",
                "# Load model for training\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=False,\n",
                ")\n",
                "\n",
                "base_model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_id,\n",
                "    quantization_config=bnb_config,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",  # Let Transformers automatically decide device placement\n",
                ")\n",
                "\n",
                "end_time = time.time()\n",
                "print(\"\\n\\n--->Execution Time:\", (end_time - start_time) / 60, \"minutes\")\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "K__xT1qcI6S2"
            },
            "source": [
                "- The method loads the specified PEFT weights or configuration associated with new_model.\n",
                "- It integrates these adaptations into the base_model, effectively modifying its architecture or parameters according to the specified PEFT approach.\n",
                "- The resulting model is now capable of utilizing the fine-tuned parameters while retaining the original capabilities of the base model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "9o7HDl8qLk_p",
                "outputId": "48cfbc34-a7a6-49ca-c786-09a1d7fd4bde"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "\n",
                        "--->Execution Time: 1.7694380203882853 minutes\n"
                    ]
                }
            ],
            "source": [
                "from peft import LoraConfig, PeftModel\n",
                "import time\n",
                "\n",
                "start_time = time.time()\n",
                "\n",
                "\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"HF_TOKEN\"])\n",
                "\n",
                "# - The method loads the specified PEFT weights associated with new_model.\n",
                "# - It integrates these adaptations into the base_model, effectively modifying its architecture or parameters according to the specified PEFT approach.\n",
                "# - The resulting model is now capable of utilizing the fine-tuned parameters while retaining the original capabilities of the base model.\n",
                "peft_model = PeftModel.from_pretrained(base_model, new_model)\n",
                "\n",
                "\n",
                "# - it combines the parameters of the LoRA layers with the corresponding layers in the base model. This effectively integrates the adaptations into the model's weights, allowing you to use the adapted model as a standalone entity\n",
                "# - After merging, the method unloads or removes any temporary components related to the LoRA layers that are no longer needed.\n",
                "merged_model = peft_model.merge_and_unload()\n",
                "\n",
                "end_time = time.time()\n",
                "print(\"\\n\\n--->Execution Time:\", (end_time - start_time) / 60, \"minutes\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "thsJNBprNvrX",
                "outputId": "29475d11-04bb-4164-ba69-f2bca1e81327"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Gemma2ForCausalLM(\n",
                        "  (model): Gemma2Model(\n",
                        "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
                        "    (layers): ModuleList(\n",
                        "      (0-45): 46 x Gemma2DecoderLayer(\n",
                        "        (self_attn): Gemma2Attention(\n",
                        "          (q_proj): Linear4bit(in_features=4608, out_features=4096, bias=False)\n",
                        "          (k_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
                        "          (v_proj): Linear4bit(in_features=4608, out_features=2048, bias=False)\n",
                        "          (o_proj): Linear4bit(in_features=4096, out_features=4608, bias=False)\n",
                        "          (rotary_emb): Gemma2RotaryEmbedding()\n",
                        "        )\n",
                        "        (mlp): Gemma2MLP(\n",
                        "          (gate_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
                        "          (up_proj): Linear4bit(in_features=4608, out_features=36864, bias=False)\n",
                        "          (down_proj): Linear4bit(in_features=36864, out_features=4608, bias=False)\n",
                        "          (act_fn): PytorchGELUTanh()\n",
                        "        )\n",
                        "        (input_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (post_attention_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (pre_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "        (post_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "      )\n",
                        "    )\n",
                        "    (norm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
                        "  )\n",
                        "  (lm_head): Linear(in_features=4608, out_features=256000, bias=False)\n",
                        ")\n"
                    ]
                }
            ],
            "source": [
                "print(merged_model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "8zFyXRTZ81EM"
            },
            "source": [
                "# **Model Evaluation using Rouge Score**\n",
                "\n",
                "More on Roughe score at https://arxiv.org/abs/1803.01937"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0R0PafHCEhk-"
            },
            "source": [
                "### Calculate Rouge Score on test data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "id": "tp9XP8tC8ASr"
            },
            "outputs": [],
            "source": [
                "from datasets import load_metric\n",
                "from rouge_score import rouge_scorer\n",
                "from datasets import load_dataset\n",
                "from datasets import DatasetDict\n",
                "import pandas as pd"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "id": "03vL37cK-Hf-"
            },
            "outputs": [],
            "source": [
                "def calculate_rouge_scores(original_summary, generated_summary):\n",
                "    rouge = load_metric(\"rouge\")\n",
                "    scores = rouge.compute(\n",
                "        predictions=[str.strip(generated_summary)], references=[original_summary]\n",
                "    )\n",
                "    return scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "GkuaFOI4KAqh"
            },
            "outputs": [],
            "source": [
                "def create_input_prompt(dialogue):\n",
                "    prompt_template = \"\"\"\n",
                "  <start_of_turn>user\n",
                "  Summarise dialogue in one sentence.\n",
                "  {dialogue}\n",
                "  <end_of_turn>\\n<start_of_turn>model\n",
                "  \"\"\"\n",
                "    prompt = prompt_template.format(dialogue=dialogue)\n",
                "    return prompt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "wxMaUU4Q3iLq",
                "outputId": "c8f74d8b-a6fb-4ecf-cab9-e6db197d93ad"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset({\n",
                        "    features: ['id', 'dialogue', 'summary'],\n",
                        "    num_rows: 819\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "# Load test data\n",
                "\n",
                "!pip install -q py7zr\n",
                "data = load_dataset(\"samsum\", split=\"test\")\n",
                "\n",
                "print(data)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "Pa5Usgh_E6Wd",
                "outputId": "e8a35a06-6932-4836-b274-901eead6c23e"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset({\n",
                        "    features: ['id', 'dialogue', 'summary'],\n",
                        "    num_rows: 25\n",
                        "})\n"
                    ]
                }
            ],
            "source": [
                "# take 25 test sample from test dataset\n",
                "\n",
                "test_dataset = data.select(range(25))\n",
                "\n",
                "print(test_dataset)\n",
                "test_dataset = pd.DataFrame(test_dataset)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000,
                    "referenced_widgets": [
                        "7b6a4e208cfc4e629c611461cde9ce05",
                        "6c1653958dcc46ffa5d67d01d6ee61f2",
                        "f2bcf7f4c7ee4375a6795c3d7cc09d62",
                        "45b1cadbfc54447b8c60a7e5a85c7199",
                        "6f62efce35264c85b96a68f795045eca",
                        "0a233cfc99264c1c87472bc1c69a4ad0",
                        "4220fe58c76545d29b218d7405e98b06",
                        "06af5fe1d757446387610a29ca688f68",
                        "0011478ef996441c85c8a2777807bcab",
                        "ada28ae5f8dd4c54922c4ed892a23c27",
                        "9e671b32aa444e44801e7839690baa1c"
                    ]
                },
                "collapsed": true,
                "id": "silz09I2EXFC",
                "jupyter": {
                    "outputs_hidden": true
                },
                "outputId": "e47c44f2-d557-48e4-a99a-6ff1f1728e4d"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The 'batch_size' attribute of HybridCache is deprecated and will be removed in v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Hannah: Hey, do you have Betty's number?\n",
                        "Amanda: Lemme check\n",
                        "Hannah: <file_gif>\n",
                        "Amanda: Sorry, can't find it.\n",
                        "Amanda: Ask Larry\n",
                        "Amanda: He called her last time we were at the park together\n",
                        "Hannah: I don't know him well\n",
                        "Hannah: <file_gif>\n",
                        "Amanda: Don't be shy, he's very nice\n",
                        "Hannah: If you say so..\n",
                        "Hannah: I'd rather you texted him\n",
                        "Amanda: Just text him 🙂\n",
                        "Hannah: Urgh.. Alright\n",
                        "Hannah: Bye\n",
                        "Amanda: Bye bye\n",
                        "  \n",
                        "model\n",
                        "  Hannah asks Amanda for Betty's number, but Amanda suggests asking Larry instead, leading to Hannah reluctantly agreeing to text him. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
                        "Generated Summary: Hannah asks Amanda for Betty's number, but Amanda suggests asking Larry instead, leading to Hannah reluctantly agreeing to text him.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<ipython-input-9-cfd78e4c7e14>:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
                        "  rouge = load_metric(\"rouge\")\n",
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "7b6a4e208cfc4e629c611461cde9ce05",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Downloading builder script:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Eric: MACHINE!\n",
                        "Rob: That's so gr8!\n",
                        "Eric: I know! And shows how Americans see Russian ;)\n",
                        "Rob: And it's really funny!\n",
                        "Eric: I know! I especially like the train part!\n",
                        "Rob: Hahaha! No one talks to the machine like that!\n",
                        "Eric: Is this his only stand-up?\n",
                        "Rob: Idk. I'll check.\n",
                        "Eric: Sure.\n",
                        "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
                        "Eric: Gr8! I'll watch them now!\n",
                        "Rob: Me too!\n",
                        "Eric: MACHINE!\n",
                        "Rob: MACHINE!\n",
                        "Eric: TTYL?\n",
                        "Rob: Sure :)\n",
                        "  \n",
                        "model\n",
                        "  Eric and Rob discuss a stand-up comedian's routine about a machine, finding it humorous and deciding to watch more of his work. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
                        "Generated Summary: Eric and Rob discuss a stand-up comedian's routine about a machine, finding it humorous and deciding to watch more of his work.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Lenny: Babe, can you help me with something?\n",
                        "Bob: Sure, what's up?\n",
                        "Lenny: Which one should I pick?\n",
                        "Bob: Send me photos\n",
                        "Lenny:  <file_photo>\n",
                        "Lenny:  <file_photo>\n",
                        "Lenny:  <file_photo>\n",
                        "Bob: I like the first ones best\n",
                        "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\n",
                        "Bob: I have four black pairs :D :D\n",
                        "Lenny: yeah, but shouldn't I pick a different color?\n",
                        "Bob: what matters is what you'll give you the most outfit options\n",
                        "Lenny: So I guess I'll buy the first or the third pair then\n",
                        "Bob: Pick the best quality then\n",
                        "Lenny: ur right, thx\n",
                        "Bob: no prob :)\n",
                        "  \n",
                        "model\n",
                        "  Lenny asks Bob for help choosing between three pairs of trousers, and Bob advises them to pick the pair that offers the most outfit options and is of the best quality. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
                        "Generated Summary: Lenny asks Bob for help choosing between three pairs of trousers, and Bob advises them to pick the pair that offers the most outfit options and is of the best quality.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Will: hey babe, what do you want for dinner tonight?\n",
                        "Emma:  gah, don't even worry about it tonight\n",
                        "Will: what do you mean? everything ok?\n",
                        "Emma: not really, but it's ok, don't worry about cooking though, I'm not hungry\n",
                        "Will: Well what time will you be home?\n",
                        "Emma: soon, hopefully\n",
                        "Will: you sure? Maybe you want me to pick you up?\n",
                        "Emma: no no it's alright. I'll be home soon, i'll tell you when I get home. \n",
                        "Will: Alright, love you. \n",
                        "Emma: love you too. \n",
                        "  \n",
                        "model\n",
                        "  Emma is having a bad day and doesn't feel like eating, but reassures Will she'll be home soon. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Emma will be home soon and she will let Will know.\n",
                        "Generated Summary: Emma is having a bad day and doesn't feel like eating, but reassures Will she'll be home soon.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Ollie: Hi , are you in Warsaw\n",
                        "Jane: yes, just back! Btw are you free for diner the 19th?\n",
                        "Ollie: nope!\n",
                        "Jane: and the  18th?\n",
                        "Ollie: nope, we have this party and you must be there, remember?\n",
                        "Jane: oh right! i lost my calendar..  thanks for reminding me\n",
                        "Ollie: we have lunch this week?\n",
                        "Jane: with pleasure!\n",
                        "Ollie: friday?\n",
                        "Jane: ok\n",
                        "Jane: what do you mean \" we don't have any more whisky!\" lol..\n",
                        "Ollie: what!!!\n",
                        "Jane: you just call me and the all thing i heard was that sentence about whisky... what's wrong with you?\n",
                        "Ollie: oh oh... very strange! i have to be carefull may be there is some spy in my mobile! lol\n",
                        "Jane: dont' worry, we'll check on friday.\n",
                        "Ollie: don't forget to bring some sun with you\n",
                        "Jane: I can't wait to be in Morocco..\n",
                        "Ollie: enjoy and see you friday\n",
                        "Jane: sorry Ollie, i'm very busy, i won't have time for lunch  tomorrow, but may be at 6pm after my courses?this trip to Morocco was so nice, but time consuming!\n",
                        "Ollie: ok for tea!\n",
                        "Jane: I'm on my way..\n",
                        "Ollie: tea is ready, did you bring the pastries?\n",
                        "Jane: I already ate them all... see you in a minute\n",
                        "Ollie: ok\n",
                        "  \n",
                        "model\n",
                        "  Ollie and Jane arrange to meet for tea instead of lunch due to Jane's busy schedule after her trip to Morocco. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
                        "Generated Summary: Ollie and Jane arrange to meet for tea instead of lunch due to Jane's busy schedule after her trip to Morocco.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Benjamin: Hey guys, what are we doing with the keys today?\n",
                        "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
                        "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
                        "Hilary: Yeah, I guess so\n",
                        "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
                        "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
                        "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
                        "Benjamin: YAAAAWN 🙊 Where and where are you meeting?\n",
                        "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
                        "Benjamin: Interesting 😱 To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
                        "Hilary: Oh come on 😂\n",
                        "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
                        "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
                        "Elliot: 🙉\n",
                        "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
                        "Elliot: Sounds like a plan 😂\n",
                        "Hilary: 😎\n",
                        "Elliot: See you at 2 then xx\n",
                        "  \n",
                        "model\n",
                        "  The group is deciding when and where to exchange keys, with Benjamin initially planning to take a nap but tempted to join Hilary for lunch with some researchers studying the history of food in colonial Mexico. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\n",
                        "Generated Summary: The group is deciding when and where to exchange keys, with Benjamin initially planning to take a nap but tempted to join Hilary for lunch with some researchers studying the history of food in colonial Mexico.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Max: Know any good sites to buy clothes from?\n",
                        "Payton: Sure :) <file_other> <file_other> <file_other> <file_other> <file_other> <file_other> <file_other>\n",
                        "Max: That's a lot of them!\n",
                        "Payton: Yeah, but they have different things so I usually buy things from 2 or 3 of them.\n",
                        "Max: I'll check them out. Thanks. \n",
                        "Payton: No problem :)\n",
                        "Max: How about u?\n",
                        "Payton: What about me?\n",
                        "Max: Do u like shopping?\n",
                        "Payton: Yes and no.\n",
                        "Max: How come?\n",
                        "Payton: I like browsing, trying on, looking in the mirror and seeing how I look, but not always buying.\n",
                        "Max: Y not?\n",
                        "Payton: Isn't it obvious? ;)\n",
                        "Max: Sry ;)\n",
                        "Payton: If I bought everything I liked, I'd have nothing left to live on ;)\n",
                        "Max: Same here, but probably different category ;)\n",
                        "Payton: Lol\n",
                        "Max: So what do u usually buy?\n",
                        "Payton: Well, I have 2 things I must struggle to resist!\n",
                        "Max: Which are?\n",
                        "Payton: Clothes, ofc ;)\n",
                        "Max: Right. And the second one?\n",
                        "Payton: Books. I absolutely love reading!\n",
                        "Max: Gr8! What books do u read?\n",
                        "Payton: Everything I can get my hands on :)\n",
                        "Max: Srsly?\n",
                        "Payton: Yup :)\n",
                        "  \n",
                        "model\n",
                        "  Max asks Payton for clothing website recommendations, and they discuss their shared love of browsing clothes but limited budgets. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
                        "Generated Summary: Max asks Payton for clothing website recommendations, and they discuss their shared love of browsing clothes but limited budgets.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Rita: I'm so bloody tired. Falling asleep at work. :-(\n",
                        "Tina: I know what you mean.\n",
                        "Tina: I keep on nodding off at my keyboard hoping that the boss doesn't notice..\n",
                        "Rita: The time just keeps on dragging on and on and on.... \n",
                        "Rita: I keep on looking at the clock and there's still 4 hours of this drudgery to go.\n",
                        "Tina: Times like these I really hate my work.\n",
                        "Rita: I'm really not cut out for this level of boredom.\n",
                        "Tina: Neither am I.\n",
                        "  \n",
                        "model\n",
                        "  Rita and Tina are both exhausted and bored at work, wishing the day would end. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Rita and Tina are bored at work and have still 4 hours left.\n",
                        "Generated Summary: Rita and Tina are both exhausted and bored at work, wishing the day would end.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Beatrice: I am in town, shopping. They have nice scarfs in the shop next to the church. Do you want one?\n",
                        "Leo: No, thanks\n",
                        "Beatrice: But you don't have a scarf.\n",
                        "Leo: Because I don't need it.\n",
                        "Beatrice: Last winter you had a cold all the time. A scarf could help.\n",
                        "Leo: I don't like them.\n",
                        "Beatrice: Actually, I don't care. You will get a scarf.\n",
                        "Leo: How understanding of you!\n",
                        "Beatrice: You were complaining the whole winter that you're going to die. I've had enough.\n",
                        "Leo: Eh.\n",
                        "  \n",
                        "model\n",
                        "  Beatrice insists on buying Leo a scarf despite his resistance, reminding him of his past suffering from colds. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\n",
                        "Generated Summary: Beatrice insists on buying Leo a scarf despite his resistance, reminding him of his past suffering from colds.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Ivan: hey eric\n",
                        "Eric: yeah man\n",
                        "Ivan: so youre coming to the wedding\n",
                        "Eric: your brother's\n",
                        "Ivan: yea\n",
                        "Eric: i dont know mannn\n",
                        "Ivan: YOU DONT KNOW??\n",
                        "Eric: i just have a lot to do at home, plus i dont know if my parents would let me\n",
                        "Ivan: ill take care of your parents\n",
                        "Eric: youre telling me you have the guts to talk to them XD\n",
                        "Ivan: thats my problem\n",
                        "Eric: okay man, if you say so\n",
                        "Ivan: yea just be there \n",
                        "Eric: alright\n",
                        "  \n",
                        "model\n",
                        "  Ivan tries to convince his hesitant friend Eric to attend his brother's wedding. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\n",
                        "Generated Summary: Ivan tries to convince his hesitant friend Eric to attend his brother's wedding.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Wanda: Let's make a party!\n",
                        "Gina: Why?\n",
                        "Wanda: beacuse. I want some fun!\n",
                        "Gina: ok, what do u need?\n",
                        "Wanda: 1st I need too make a list\n",
                        "Gina: noted and then?\n",
                        "Wanda: well, could u take yours father car and go do groceries with me?\n",
                        "Gina: don't know if he'll agree\n",
                        "Wanda: I know, but u can ask :)\n",
                        "Gina: I'll try but theres no promisess\n",
                        "Wanda: I know, u r the best!\n",
                        "Gina: When u wanna go\n",
                        "Wanda: Friday?\n",
                        "Gina: ok, I'll ask\n",
                        "  \n",
                        "model\n",
                        "  Wanda wants to throw a party and asks Gina to help with groceries, potentially using her father's car. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday. \n",
                        "Generated Summary: Wanda wants to throw a party and asks Gina to help with groceries, potentially using her father's car.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Martin: I won two cinema tickets!\n",
                        "Aggie: oh cool, how come?\n",
                        "Martin: online. on fb, the movie mag organized it\n",
                        "Aggie: so what did you do\n",
                        "Martin: just write a short review and that's it\n",
                        "Aggie: well done :) so what and when. and where?\n",
                        "Martin: the new film with Redford\n",
                        "Aggie: i guess i heard sth\n",
                        "Martin: it's pretty cool i heard. till the end of the week\n",
                        "Aggie: sounds good. we'll find time XD\n",
                        "  \n",
                        "model\n",
                        "  Martin won two cinema tickets to a new Redford film by writing a short review online and he and Aggie plan to see it together. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Martin wrote a short review and won 2 cinema tickets on FB. Martin wants Aggie to go with him this week for the new film with Redford.\n",
                        "Generated Summary: Martin won two cinema tickets to a new Redford film by writing a short review online and he and Aggie plan to see it together.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Charlee: I'm in class. Theatre in Portuguese lol\n",
                        "Curtis: Realllly?\n",
                        "Charlee: Yes. One of my subjects at the university that I attend is portuguese theatre. We are preparing a performance\n",
                        "Curtis: What performance is this? Are you devising it?\n",
                        "Charlee: A polish one translated into portuguese\n",
                        "Curtis: Thats quite cool. Who is the writer?\n",
                        "Charlee: Mrożek\n",
                        "  \n",
                        "model\n",
                        "  Charlee tells Curtis about her Portuguese theatre class where they are preparing a performance of a Polish play translated into Portuguese by Mrożek. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Charlee is attending Portuguese theater as a subject at university. He and other students are preparing a play by Mrożek translated into Portuguese.\n",
                        "Generated Summary: Charlee tells Curtis about her Portuguese theatre class where they are preparing a performance of a Polish play translated into Portuguese by Mrożek.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Mary: Are you going by car or train?\n",
                        "Tom: Ella rented a car\n",
                        "Ella: this makes all of this much faster\n",
                        "Mary: good decision\n",
                        "  \n",
                        "model\n",
                        "  Ella rented a car, which will make their trip faster. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Ella rented a car, this makes things much faster for her and Tom. \n",
                        "Generated Summary: Ella rented a car, which will make their trip faster.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Luke: are you still looking for someone to join netflix family?\n",
                        "Paul: yes, 1 person :)\n",
                        "Luke: i am the one!\n",
                        "Paul: sure, i will send you the login and password on sunday\n",
                        "Luke: ok we can talk tomorrow\n",
                        "Paul: i don't really remember it now\n",
                        "Luke: send me also the bank account details so I can wire you the money every month. Are you paying for this or someone else?\n",
                        "Paul: I do, and I keep track of everyone accessing so you should not expect any bans :D\n",
                        "Luke: easy mate :D you still on holidays with your girl?\n",
                        "Paul: last dinner :( tomorrow we are out\n",
                        "Luke: how long have you been there?\n",
                        "Paul: less than 8 days :/\n",
                        "  \n",
                        "model\n",
                        "  Luke agrees to join Paul's Netflix family plan and will pay him monthly. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Paul is going to share his Netflix account with Luke. In exchange Luke is going to contribute to the subscription. Paul will send Luke his bank details. Paul is on vacation with his girlfriend till tomorrow.\n",
                        "Generated Summary: Luke agrees to join Paul's Netflix family plan and will pay him monthly.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Greg: Hi, honey. I need to stay after hours :-(\n",
                        "Betsy: Again?\n",
                        "Greg: I'm sorry!\n",
                        "Betsy: What about Johnny?\n",
                        "Greg: Well, could you pick him up? \n",
                        "Betsy: What if I can't?\n",
                        "Greg: Betsy?\n",
                        "Betsy: What if I can't?\n",
                        "Greg: Can't you, really?\n",
                        "Betsy: I can't. Today I need to work long hours as well. Tuesdays are your days in the kindergarten.\n",
                        "Greg: Talk to you later. I'll see what I can do.\n",
                        "Betsy: You'd better think of something.\n",
                        "Greg: Oh. Just stop it now.\n",
                        "  \n",
                        "model\n",
                        "  Greg needs Betsy to pick up their son Johnny from kindergarten because he has to work late, but Betsy is also working late and reminds Greg it's his day for kindergarten pickup. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Greg and Betsy have a lot of work today, so they cannot pick up Johnny from the kindergarten. However, it's Greg's turn to do it. Greg will try to find a solution.\n",
                        "Generated Summary: Greg needs Betsy to pick up their son Johnny from kindergarten because he has to work late, but Betsy is also working late and reminds Greg it's his day for kindergarten pickup.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Ethan: somethin for Scott <file_photo>\n",
                        "Toby: haha, totally\n",
                        "Marshall: pretty much sums it up\n",
                        "Scott: you know you're exactly fuckin the same\n",
                        "Toby: oh we know honey bunny\n",
                        "Marshall: we just enjoy making fun of YOU\n",
                        "Ethan: xD\n",
                        "Scott: oh fuck y'all\n",
                        "Toby: <file_gif>\n",
                        "  \n",
                        "model\n",
                        "  Ethan, Toby, and Marshall are teasing Scott about his personality. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Ethan, Toby and Marshall are making fun of Scott.\n",
                        "Generated Summary: Ethan, Toby, and Marshall are teasing Scott about his personality.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Igor: Shit, I've got so much to do at work and I'm so demotivated. \n",
                        "John: It's pretty irresponsible to give that much work to someone on their notice period.\n",
                        "Igor: Yeah, exactly! Should I even care?\n",
                        "John: It's up to you, but you know what they say...\n",
                        "Igor: What do you mean?\n",
                        "John: Well, they say how you end things shows how you really are...\n",
                        "Igor: And now how you start, right?\n",
                        "John: Gotcha! \n",
                        "Igor: So what shall I do then? \n",
                        "John: It's only two weeks left, so grit your teeth and do what you have to do. \n",
                        "Igor: Easy to say, hard to perform.\n",
                        "John: Come on, stop thinking, start doing! \n",
                        "Igor: That's so typical of you!  ;)  \n",
                        "  \n",
                        "model\n",
                        "  Igor is feeling overwhelmed by work during his notice period and John encourages him to finish strong despite his lack of motivation. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Igor has a lot of work on his notice period and he feels demotivated. John thinks he should do what he has to do nevertheless. \n",
                        "Generated Summary: Igor is feeling overwhelmed by work during his notice period and John encourages him to finish strong despite his lack of motivation.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Clara: Hi, what you up to?\n",
                        "Neela: Not much, chilling out.\n",
                        "Clara: Just rewatching Dear White People on Netflix, love it!😍\n",
                        "Neela: Oh yeah, heard of it, but not seen it yet? Any good?\n",
                        "Clara: Well, yes! I just said it was, LOL. It's about a fictional Ivy League University and the students in one House of Residence.\n",
                        "Neela: Why is it called Dear White People?\n",
                        "Clara: That's the name of the radio show the main character, Sam, presents on college radio.\n",
                        "Neela: Yeah, but why is it so good?\n",
                        "Clara: Well, it's mainly stories from the perspective of black students there, which I find very interesting. The characters are strong and likeable too.\n",
                        "Neela: I suppose it's rather different from the UK, then?\n",
                        "Clara: It seems so, as there is a lot more racial awareness and discrimination there than here. It all kicks off when there is a Blackface party held by an elite group of white students, which gets out of hand.\n",
                        "Neela: How's that?\n",
                        "Clara: Well, obviously, the black students try to break it up and there's also an incident where one guy, Reggie, gets a loaded gun pointed at him by a campus policeman after he gets into an argument with a white student. It may be at another party, though, I'm not sure of that.\n",
                        "Neela: Oh, that sounds pretty strong stuff. What else happens?\n",
                        "Clara: Well, there is a young black guy called Lionel who is coming to terms with being gay and is finding his voice as a journalist. He unearths corruption at the uni and he and Sam also uncover some conspiracy theory stuff about secret societies.\n",
                        "Neela: Well, I must say, it does sound good, I'll check it out soon!\n",
                        "Clara: Definitely, there is supposed to be a Series 3 coming up next year, really looking forward to it!\n",
                        "Neela: Well, thanks Clara, I'm just watching the rest of a movie and I'll try Dear White People.\n",
                        "Clara: Don't blame me if you get hooked and stay up till 4!\n",
                        "Neela: See ya, love!\n",
                        "Clara: Bye!\n",
                        "  \n",
                        "model\n",
                        "  Clara recommends the Netflix show \"Dear White People\" to Neela, describing its plot about racial dynamics and student life at a fictional Ivy League university. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Clara is rewatching Dear White People and strongly recommends it to Neela.\n",
                        "Generated Summary: Clara recommends the Netflix show \"Dear White People\" to Neela, describing its plot about racial dynamics and student life at a fictional Ivy League university.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Ernest: hey Mike, did you park your car on our street?\n",
                        "Mike: no, took it into garage today\n",
                        "Ernest: ok good\n",
                        "Mike: why?\n",
                        "Ernest: someone just crashed into a red honda looking just like yours\n",
                        "Mike: lol lucky me\n",
                        "  \n",
                        "model\n",
                        "  Ernest warns Mike that a car resembling his was just crashed into, but Mike reveals he parked his car in the garage. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Mike took his car into garage today. Ernest is relieved as someone had just crashed into a red Honda which looks like Mike's. \n",
                        "Generated Summary: Ernest warns Mike that a car resembling his was just crashed into, but Mike reveals he parked his car in the garage.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Deirdre: Hi Beth, how are you love?\n",
                        "Beth: Hi Auntie Deirdre, I'm been meaning to message you, had a favour to ask.\n",
                        "Deirdre: Wondered if you had any thought about your Mum's 40th, we've got to do something special!\n",
                        "Beth: How about a girls weekend, just mum, me, you and the girls, Kira will have to come back from Uni, of course.\n",
                        "Deirdre: Sounds fab! Get your thinking cap on, it's only in 6 weeks! Bet she's dreading it, I remember doing that!\n",
                        "Beth: Oh yeah, we had a surprise party for you, you nearly had a heart attack! \n",
                        "Deirdre: Well, it was a lovely surprise! Gosh, thats nearly 4 years ago now, time flies! What was the favour, darling?\n",
                        "Beth: Oh, it was just that I fancied trying a bit of work experience in the salon, auntie.\n",
                        "Deirdre: Well, I am looking for Saturday girls, are you sure about it? you could do well in the exams and go on to college or 6th form.\n",
                        "Beth: I know, but it's not for me, auntie, I am doing all foundation papers and I'm struggling with those.\n",
                        "Deirdre: What about a tutor? Kira could help you in the hols.\n",
                        "Beth: Maybe, but I'd like to try working. I'm 16 soon, I'm old enough.\n",
                        "Deirdre: I know. Look, pop in tomorrow after school and we'll have a cuppa and a chat.\n",
                        "Beth: Yes, thanks auntie. I'd really like to try the beauty therapy side.\n",
                        "Deirdre: Its not for the squeamish, mind. Massage, pedicures, not to mention waxing!\n",
                        "Beth: Oh yes, I was chatting to a friend about it yesterday!\n",
                        "Deirdre: Maxine manages the beauty side, you can meet her tomorrow and we'll see how it goes.\n",
                        "Beth: Yes, I'd really like that. \n",
                        "Deirdre: We can try a few hours on a Saturday for a couple of weeks as work experience. I'll give you a tenner or so per session to start off for your lunch, coffee and bus fare etc. If you like, we'll take it from there.\n",
                        "Beth: OK, I like the sound of it! See you tomorrow Auntie! Love you!\n",
                        "Deirdre: Bye, lovely girl! Xx\n",
                        "  \n",
                        "model\n",
                        "  Beth asks her aunt Deirdre for work experience at her salon, and Deirdre agrees to let her try out the beauty therapy side for a few hours on Saturdays. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Beth wants to organize a girls weekend to celebrate her mother's 40th birthday. She also wants to work at Deidre's beauty salon. Deidre offers her a few hours on Saturdays as work experience. They set up for a meeting tomorrow.\n",
                        "Generated Summary: Beth asks her aunt Deirdre for work experience at her salon, and Deirdre agrees to let her try out the beauty therapy side for a few hours on Saturdays.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Gloria: This exam is a bit of a lottery in fact\n",
                        "Gloria: You can't really get prepared, it's all about experience\n",
                        "Emma: But there are some rules and some typical texts right?\n",
                        "Gloria: You can see some texts from previous years\n",
                        "Gloria: <file_other>\n",
                        "Emma: Wow that's very useful\n",
                        "Emma: I have never seen this site\n",
                        "Gloria: Yes it's very good\n",
                        "Gloria: Actually it's good to read all the texts because you will see that some phrases repeat very often\n",
                        "Emma: How much time do you have for all 4 parts?\n",
                        "Gloria: 4 hours\n",
                        "Emma: Is it enough?\n",
                        "Gloria: Well it has to be\n",
                        "Gloria: Would be perfect to have 2 more hours... But on the other hand it would be really exhausting\n",
                        "Emma: 4 hours and no breaks?\n",
                        "Gloria: No breaks :/ So it's really important to be really focused and try to write as fast as you can\n",
                        "Gloria: And read it carefully and correct during the last hour\n",
                        "Emma: I'm going to read everything from that website, it's great\n",
                        "  \n",
                        "model\n",
                        "  Gloria advises Emma on how to prepare for a challenging exam, emphasizing the importance of reviewing past texts and managing time effectively within the four-hour limit. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Gloria has an exam soon. It lasts 4 hours. Emma sent her a link to a website with some texts from previous years so that she can prepare for the exam better.\n",
                        "Generated Summary: Gloria advises Emma on how to prepare for a challenging exam, emphasizing the importance of reviewing past texts and managing time effectively within the four-hour limit.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Adam: Have you talked to May?\n",
                        "Karen: Yes, yesterday, why?\n",
                        "Adam: I just talked to her and I must admit I worry about her\n",
                        "Karen: Me too, I suggested she should see a specialist, but she wasn't very happy about it\n",
                        "Adam: No wonder...\n",
                        "Karen: I know, but I think this is serious. She's saying she's depressed, like everyone around, but in her case it may be true\n",
                        "Adam: She was telling me she doesn't feel like doing anything, she's bored all the time, she never feels happy. It sounds like a real, typical depression\n",
                        "Adam: She also told me that she has trouble sleeping. I asked her to go out for a beer or anything basically, but she doesn't want to leave the flat\n",
                        "Karen: Oh my, it sounds really serious. I don't what to tell you\n",
                        "Adam: I was wondering how I can help her\n",
                        "Karen: Honestly I don't know if we can help her, Adam. I suggested a specialist because these are very sensitive issues and I'm afraid we may unintentionally make it worse\n",
                        "Adam: Yes, but she doesn't want to see a specialist. Basically, she doesn't want to see anyone\n",
                        "Karen: Hm... I don't know... How about I call someone for advice? So we could know what to do\n",
                        "Adam: Sounds rational, do you know anyone you could call? Don't mention her name\n",
                        "Karen: Of course I won't! I have a friend who's a psychologist, we can trust her. I'll let you know\n",
                        "Adam: Thank you Karen!\n",
                        "  \n",
                        "model\n",
                        "  Adam and Karen are worried about May's depression and are seeking advice on how to help her. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Adam and Karen are worried that May suffers from depression. Karen will call her friend who is a psychologist and ask for advice. \n",
                        "Generated Summary: Adam and Karen are worried about May's depression and are seeking advice on how to help her.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Anne: You were right, he was lying to me :/\n",
                        "Irene: Oh no, what happened?\n",
                        "Jane: who? that Mark guy?\n",
                        "Anne: yeah, he told me he's 30, today I saw his passport - he's 40\n",
                        "Irene: You sure it's so important?\n",
                        "Anne: he lied to me Irene\n",
                        "  \n",
                        "model\n",
                        "  Anne discovered her boyfriend Mark lied about his age, causing her distress. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Mark lied to Anne about his age. Mark is 40.\n",
                        "Generated Summary: Anne discovered her boyfriend Mark lied about his age, causing her distress.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "  user\n",
                        "  Summarise dialogue in one sentence.\n",
                        "  Augustine: Guys, remember it's Wharton's bday next week?\n",
                        "Darlene: yay, a party!\n",
                        "Heather: yay! crap we need to buy him a present\n",
                        "Walker: he mentioned paper shredder once\n",
                        "Augustine: wtf?!?\n",
                        "Walker: he did really. for no reason at all.\n",
                        "Heather: whatever that make him happy\n",
                        "Darlene: cool with me. we can shred some papers at the party \n",
                        "Augustine: so much fun\n",
                        "Heather: srsly guys, you mean we should really get office equipment???\n",
                        "Darlene: Walk, ask him if he really wnts it and if he yes then we get it\n",
                        "Walker: i heard him say that. wasn;t drunk. me neither.\n",
                        "Darlene: but better ask him twice\n",
                        "Walker: will do\n",
                        "Augustine: 2moro ok?\n",
                        "Darlene: and sure ask ab the party!\n",
                        "  \n",
                        "model\n",
                        "  The group decides to ask Wharton if he really wants a paper shredder for his birthday present. \n",
                        "\n",
                        "\n",
                        "---------------------------------------------------------------------\n",
                        "True Summary: Next week is Wharton's birthday. Augustine, Darlene, Heather and Walker want to buy him a paper shredder. Walker will make sure if Wharton really wants it. \n",
                        "Generated Summary: The group decides to ask Wharton if he really wants a paper shredder for his birthday present.\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.10/dist-packages/datasets/load.py:753: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.17.0/metrics/rouge/rouge.py\n",
                        "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
                        "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "num_iterations = len(test_dataset)\n",
                "\n",
                "avg_scores = {\n",
                "    \"rouge1\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
                "    \"rouge2\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
                "    \"rougeL\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
                "    \"rougeLsum\": {\"precision\": 0, \"recall\": 0, \"f1\": 0},\n",
                "}\n",
                "\n",
                "\n",
                "for idx, row in test_dataset.iterrows():\n",
                "    device = \"cuda:0\"\n",
                "    dialogue = row[\"dialogue\"]\n",
                "    true_summary = row[\"summary\"]\n",
                "\n",
                "    text = create_input_prompt(dialogue)  # convert into gemma prompt template\n",
                "\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
                "    outputs = merged_model.generate(**inputs, max_new_tokens=50)\n",
                "    gemma_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "    print(gemma_summary)\n",
                "\n",
                "    print(\"---------------------------------------------------------------------\")\n",
                "    print(f\"True Summary: {true_summary}\")\n",
                "\n",
                "    # end_token = \"\"\n",
                "\n",
                "    highlight = str.strip(gemma_summary.split(\"model\")[1])\n",
                "    print(f\"Generated Summary: {highlight}\")\n",
                "    print()\n",
                "\n",
                "    ## rouge score\n",
                "    rouge_scores = calculate_rouge_scores(highlight, true_summary)\n",
                "    rouge_scorer_ = rouge_scorer.RougeScorer(\n",
                "        [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n",
                "    )\n",
                "    rouge_scores = rouge_scorer_.score(highlight, true_summary)\n",
                "\n",
                "    for metric, scores in rouge_scores.items():\n",
                "        rouge_scores_matrix = {\n",
                "            metric: {\n",
                "                \"precision\": scores.precision,\n",
                "                \"recall\": scores.recall,\n",
                "                \"fmeasure\": scores.fmeasure,\n",
                "            }\n",
                "        }\n",
                "        # Convert the rouge_scores to a DataFrame\n",
                "        df = pd.DataFrame(rouge_scores_matrix).transpose()\n",
                "        # print(df)\n",
                "\n",
                "        avg_scores[metric][\"precision\"] += scores.precision\n",
                "        avg_scores[metric][\"recall\"] += scores.recall\n",
                "        avg_scores[metric][\"f1\"] += scores.fmeasure\n",
                "\n",
                "\n",
                "for metric, scores in avg_scores.items():\n",
                "    avg_scores[metric][\"precision\"] /= num_iterations\n",
                "    avg_scores[metric][\"recall\"] /= num_iterations\n",
                "    avg_scores[metric][\"f1\"] /= num_iterations\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "4E0fdHiFQnx5"
            },
            "source": [
                "### Average rouge score on test data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "jjahTAJMHTfk",
                "outputId": "966e0fc6-f02e-4e59-93a8-ebcba7c22a10"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test dataset average rouge score...\n",
                        "           precision    recall        f1\n",
                        "rouge1      0.432926  0.460685  0.430851\n",
                        "rouge2      0.190036  0.189437  0.183169\n",
                        "rougeL      0.336717  0.355725  0.334290\n",
                        "rougeLsum   0.336717  0.355725  0.334290\n"
                    ]
                }
            ],
            "source": [
                "# Convert the evaluation results to a DataFrame\n",
                "df = pd.DataFrame(avg_scores)\n",
                "\n",
                "# Transpose the DataFrame for better readability\n",
                "df = df.transpose()\n",
                "\n",
                "# Print the DataFrame\n",
                "print(\"Test dataset average rouge score...\")\n",
                "print(df)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "1Tool4stI6S8"
            },
            "source": [
                "The ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metrics are commonly used to evaluate the quality of summaries generated by models. They compare the overlap of n-grams between the generated summary and reference summaries. Here, you have ROUGE-1, ROUGE-2, ROUGE-L, and ROUGE-Lsum scores, each with precision, recall, and F1 metrics.\n",
                "\n",
                "\n",
                "**Interpretation of ROUGE Scores**\n",
                "\n",
                "1. **ROUGE-1**:\n",
                "   - **Precision**: 0.432926 (43.29% of the unigrams in the generated summary are also in the reference summary).\n",
                "   - **Recall**: 0.460685 (46.07% of the unigrams in the reference summary are also in the generated summary).\n",
                "   - **F1 Score**: 0.430851 (Balanced measure of precision and recall).\n",
                "\n",
                "2. **ROUGE-2**:\n",
                "   - **Precision**: 0.190036 (19.00% of the bigrams in the generated summary are also in the reference summary).\n",
                "   - **Recall**: 0.189437 (18.94% of the bigrams in the reference summary are also in the generated summary).\n",
                "   - **F1 Score**: 0.183169 (Balanced measure of precision and recall).\n",
                "\n",
                "3. **ROUGE-L**:\n",
                "   - **Precision**: 0.336717 (33.67% of the longest common subsequences in the generated summary are also in the reference summary).\n",
                "   - **Recall**: 0.355725 (35.57% of the longest common subsequences in the reference summary are also in the generated summary).\n",
                "   - **F1 Score**: 0.334290 (Balanced measure of precision and recall).\n",
                "\n",
                "4. **ROUGE-Lsum**:\n",
                "   - **Precision**: 0.336717 (Same as ROUGE-L).\n",
                "   - **Recall**: 0.355725 (Same as ROUGE-L).\n",
                "   - **F1 Score**: 0.334290 (Same as ROUGE-L).\n",
                "\n",
                "### Analysis\n",
                "\n",
                "1. **ROUGE-1** scores are relatively higher, indicating that the generated summary captures a good portion of the important words from the reference summary.\n",
                "2. **ROUGE-2** scores are lower, suggesting that the model struggles more with capturing the correct sequence of words (bigrams).\n",
                "3. **ROUGE-L** and **ROUGE-Lsum** scores are in between, indicating that the model captures some of the longer sequences of words correctly but still has room for improvement.\n",
                "\n",
                "**Recommendations for Improvement**\n",
                "\n",
                "1. **Improve Bigram Capture**:\n",
                "   - Focus on improving the model's ability to capture bigrams, as indicated by the lower ROUGE-2 scores. This can be achieved by fine-tuning the model further or using more sophisticated training techniques.\n",
                "\n",
                "2. **Fine-Tuning**:\n",
                "   - Further fine-tuning the model with more data or using techniques like data augmentation might help improve the scores.\n",
                "\n",
                "3. **Model Architecture**:\n",
                "   - Experiment with different model architectures or hyperparameters to see if they yield better results.\n",
                "\n",
                "4. **Regularization**:\n",
                "   - Implement regularization techniques such as dropout and weight decay to prevent overfitting and improve generalization.\n",
                "\n",
                "5. **Learning Rate Adjustment**:\n",
                "   - Experiment with different learning rates and learning rate schedules to find the optimal value that allows the model to learn effectively without overshooting.\n",
                "\n",
                "\n",
                "\n",
                "**Summary**\n",
                "\n",
                "- **ROUGE-1**: Indicates good capture of important words.\n",
                "- **ROUGE-2**: Indicates struggles with capturing correct sequences of words.\n",
                "- **ROUGE-L** and **ROUGE-Lsum**: Indicates some capture of longer sequences but room for improvement.\n",
                "- **Suggestions**: Improve bigram capture, fine-tune further, experiment with model architectures, implement regularization, and adjust learning rates.\n",
                "\n",
                "By making these adjustments, you can improve the model's performance and generalization, leading to better ROUGE scores and overall summarization quality."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 86
                },
                "collapsed": true,
                "id": "fVLoym9jMHWc",
                "jupyter": {
                    "outputs_hidden": true
                },
                "outputId": "14f15333-25ce-414e-9b6c-be5bc2b222af"
            },
            "outputs": [
                {
                    "data": {
                        "text/html": [
                            " View run <strong style=\"color:#cdcd00\">ethereal-energy-13</strong> at: <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v2/runs/o3v9kbds' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v2/runs/o3v9kbds</a><br/> View project at: <a href='https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v2' target=\"_blank\">https://wandb.ai/pratik_ai/gemma-2-27b-it_ft_summarizer_v2</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "text/html": [
                            "Find logs at: <code>./wandb/run-20241211_102733-o3v9kbds/logs</code>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "wandb.finish()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "MmR3VG0C1RHj"
            },
            "source": [
                "# Push Model to Huggingface hub"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 229,
                    "referenced_widgets": [
                        "fe43ce347038419a959ddb4c85cb94f6",
                        "31cc17e9e9b24ccf93c6b1dcb36345c8",
                        "872a0039b51f4ad88ab163e714b30fd9",
                        "3f596369170f45f1bd3cc4570f115cac",
                        "0b9f7308b8594f4d8316295965c0ab5a",
                        "91b5564c01c24c9fb1773bbd1d1c07e7",
                        "21c84a5c6eea4dc0949d7487214e2e97",
                        "55b77bf7ac914be7a088deecb63e5919",
                        "ef4794008c694ddd9c4de2a4d43dc8e2",
                        "16e2a657f5ec4f7a8637a356a020e986",
                        "74dfd9efa77c435ea79d7a11ffda2be8",
                        "0f571466b9d14d9a9478d75636a88c68",
                        "2cd756f304d945a8993f554bacc10362",
                        "d6adbd3766934cefa2c52f2a96524e0e",
                        "48b62946d1484880a1f3b7186af1b705",
                        "102302661a8c49d1a8da4e0881ceb010",
                        "04e9b423818341f084eaf4b00c695e4c",
                        "5f39e5426aed4196bf63a2eb2aba8e6f",
                        "9004186bbb8542a99aeb6a03dbf38725",
                        "ee4cd4c2c7b24171a52927a00c5721e8",
                        "6aa47efadc2e40779c9205939edfff73",
                        "7caab412228140f19a886518c9a73ba1",
                        "1d84975023314ea6b7a054078f710bfb",
                        "afee9716a60449059b8ade5508ad854e",
                        "35cb74b8cdfc40d5a4857db0ce7afb1d",
                        "ef4914c265104f71b4a8a90285f06872",
                        "948747bb597c45b4995953a95bb6a5ef",
                        "a96648ecea6d4effa3464456426ca3d2",
                        "60c0d68e4a8a42bcba347498577e9863",
                        "1844333ce5c04eaca80291b25fba9090",
                        "f47174b846e14f1e92a91360880cff18",
                        "20f48abac1f342cf8de78645f335efe9",
                        "6778b0bc888b460db146e1f074c181c6",
                        "180000bd0ef1499e91bf7dde0ac48826",
                        "30054e15f49d41cb8302fad5a52fa8e0",
                        "be22776350fc4021adc3fefdfc5c977b",
                        "d66c15a7af2b44fc9e69b59a1254de18",
                        "c5bc4dec85cc43a6b2f86d9a51a6674b",
                        "bc4338043cf64dcd890c50bfb635fadc",
                        "4db2248c51594206a79b7f47107fc87d",
                        "9a233dfc691944ecaf17d23ae2952b0a",
                        "07097e0f5fa94dc88fb6f11439444ed0",
                        "5d2e51d7c78d4fb4823a57d962d5abab",
                        "d802a201f86443588e406a32b82ed337",
                        "c5f5011ed3af407eac776842b1416a0e",
                        "95cdd93e5bdc4f3fb38165d0de54874c",
                        "ad39d22f86c848a0ba70d82049df34aa",
                        "2ed9f4043a2b429184024a13ab5aa34a",
                        "0579924b13f146d397a96e52b5613c04",
                        "cc160bdc2c514ed9985a8435df68222a",
                        "03499f85c702444a80cacf454d030188",
                        "6a6c10ae301a441f9b5d1a8b1d6525cd",
                        "dea90112df33412db01ae720d6b9fca5",
                        "36d3a2cb16ca489381e42027cc1c0398",
                        "6640df32a0c4446ea775bae811bcc8b6"
                    ]
                },
                "id": "wbWApexv1OM7",
                "outputId": "436ec4a5-e39f-4d8b-86b0-c34f646ff8ca"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "fe43ce347038419a959ddb4c85cb94f6",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "0f571466b9d14d9a9478d75636a88c68",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "1d84975023314ea6b7a054078f710bfb",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "180000bd0ef1499e91bf7dde0ac48826",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00001-of-00004.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "c5f5011ed3af407eac776842b1416a0e",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model-00004-of-00004.safetensors:   0%|          | 0.00/2.11G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.google.colaboratory.intrinsic+json": {
                            "type": "string"
                        },
                        "text/plain": [
                            "CommitInfo(commit_url='https://huggingface.co/Prat/gemma-2-27b-it-ft-summarizer-v3/commit/4f6107af4d3268fc8a2aaa7267ea3fa7f9ded6eb', commit_message='Upload Gemma2ForCausalLM', commit_description='', oid='4f6107af4d3268fc8a2aaa7267ea3fa7f9ded6eb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Prat/gemma-2-27b-it-ft-summarizer-v3', endpoint='https://huggingface.co', repo_type='model', repo_id='Prat/gemma-2-27b-it-ft-summarizer-v3'), pr_revision=None, pr_num=None)"
                        ]
                    },
                    "execution_count": 16,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "merged_model.save_pretrained(\"gemma-2-27b-it-ft-summarizer-v3\")\n",
                "merged_model.push_to_hub(\"gemma-2-27b-it-ft-summarizer-v3\", use_temp_dir=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "qODwO4RAL9x8"
            },
            "source": [
                "# **Thank You!!**"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "A100",
            "machine_shape": "hm",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.7"
        }
       
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
