{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDdY2Xatywjg"
      },
      "source": [
        "\n",
        "\n",
        "# **Distributed finetuning ```gemma-2-9b-it``` model using ```LORA``` on ```SAMSum dataset``` (abstractive dialogue summaries)**\n",
        "\n",
        "\n",
        "Following adjustments make to fit large model (9 billion parameters) in memory during finetuning :\n",
        "\n",
        "- ```Use Mixed Precision Training```: This reduces memory usage and speeds up training.\n",
        "- ```Gradient Checkpointing```: This trades compute for memory by recomputing activations during the backward pass.\n",
        "- ```gradient_accumulation_steps```: By using gradient_accumulation_steps, you can train models with larger effective batch sizes without running into memory limitations, leading to potentially better model performance and training stability.\n",
        "- ```Reduce Batch Size```: Smaller batch sizes reduce memory usage.\n",
        "- ```Offload to CPU```: Offload parts of the model to the CPU to save GPU memory.\n",
        "- ```Use Efficient Optimizers```: Use optimizers that are memory efficient.\n",
        "- ```max_seq_length```: Use small max_seq_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lISpHHuLAdD"
      },
      "source": [
        "# **Import Libs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SD6A_C-YXhE2",
        "outputId": "cbced831-85ed-4b44-ea25-0315c6cf0149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "trl 0.12.1 requires datasets>=2.21.0, but you have datasets 2.17.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -q -U accelerate\n",
        "!pip3 install -q -U bitsandbytes\n",
        "!pip3 install -q -U peft\n",
        "!pip3 install -q -U trl\n",
        "!pip3 install -q -U accelerate\n",
        "!pip3 install -q -U datasets==2.17.0\n",
        "!pip3 install -q -U transformers\n",
        "!pip install -q rouge_score\n",
        "!pip install -q optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaE3AtAFyC7e",
        "outputId": "19bdab19-9152-4af6-eb5c-7fb4ebda1631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is CUDA available?  True\n",
            "Device name:  Tesla T4\n",
            "Wed Dec  4 11:17:54 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8              10W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "print(\"Is CUDA available? \", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Device name: \", torch.cuda.get_device_name(0))\n",
        "    !nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Hy0GBC_tWYS0"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "from datasets import load_dataset\n",
        "from datasets import load_metric\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import transformers\n",
        "from trl import SFTTrainer\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rb9mDO0iWt1v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = \"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\"\n",
        "# os.environ[\"WEIGHT_BIASES\"] = \"9d7decf681236b200a35c0121bca0fe725be724c\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_tN-wYlKdi3"
      },
      "source": [
        "# **Load Model and tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fvQLkb9bW2PE"
      },
      "outputs": [],
      "source": [
        "# load a pre-trained tokenizer from the Hugging Face Model Hub, with authentication for the Hugging Face API token\n",
        "\n",
        "\n",
        "model_id = \"google/gemma-2-9b-it\"\n",
        "new_model = \"Dist_gemma-2-9b-it_summarizer_v2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"HF_TOKEN\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7SKrjH8faf-"
      },
      "source": [
        "# **LORA-Finetuning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9iK1mibgRk7"
      },
      "source": [
        "## **Load Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjS8Hwy0gN07",
        "outputId": "149e04b2-9f88-4c5b-b4ab-ccb997299820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1454: FutureWarning: The repository for samsum contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/samsum\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "## list of dataset for summarization. Choose one of them for your task\n",
        "# https://paperswithcode.com/dataset/cnn-daily-mail-1\n",
        "# data = load_dataset(\"knkarthick/dialogsum\") ##Dialogue Summarization Dataset\n",
        "# data = load_dataset(\"cnn_dailymail\",\"3.0.0\")\n",
        "# data = load_dataset(\"GEM/wiki_lingua\")\n",
        "\n",
        "\n",
        "!pip install -q py7zr\n",
        "data = load_dataset(\"samsum\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zdj4BkZ-gXpU",
        "outputId": "45cf50ca-c5e6-4d0b-dcf3-8123ae65e6cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1XLPj02gap8",
        "outputId": "af9dd977-a1a9-4cf7-e618-e1aab99ae4e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': '13818513',\n",
              " 'dialogue': \"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
              " 'summary': 'Amanda baked cookies and will bring Jerry some tomorrow.'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne0RAbkvHj3h",
        "outputId": "bb27683c-62ec-459e-c2a0-8b62f1adda33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['id', 'dialogue', 'summary'],\n",
            "    num_rows: 100\n",
            "})\n",
            "Dataset({\n",
            "    features: ['id', 'dialogue', 'summary'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "DATA_RECORD_SIZE = 100  # size of training dataset\n",
        "\n",
        "dataset_dict = DatasetDict(data)\n",
        "# Extract the first 100 rows from the training dataset\n",
        "training_dataset = dataset_dict[\"train\"].select(range(DATA_RECORD_SIZE))\n",
        "\n",
        "# Extract the first 100 rows from the training dataset\n",
        "val_dataset = dataset_dict[\"validation\"].select(range(DATA_RECORD_SIZE))\n",
        "\n",
        "print(training_dataset)\n",
        "print(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_dataset[\"dialogue\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JO1JR7i_SzPn",
        "outputId": "5a81b5f2-b454-4c0a-9b5e-41bcd2f882b1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Model"
      ],
      "metadata": {
        "id": "F40qHhqSjEX_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "449b7c3edcd340efa7f8246a374b216a",
            "fbf5052fed994b528114fe03bcaa39b9",
            "997b3db3b16149bb948ee5724e803a7f",
            "eb9fedf9246941d2b8330317cd5d3662",
            "bcaa0b9d381249c78fb4a215e3929308",
            "40d89b7aaf89479c804ba706bd307017",
            "30341ad92b7f4cc8a174f8b06c9d8ed7",
            "658e431aab0d47ffae04955ed3d9f4a7",
            "c497f05d31204d30b45e9fc009d79184",
            "19cbb774e3ac4d7d9af51392a568e589",
            "89815b317acd41a7a7afaa23eb022c7f"
          ]
        },
        "id": "5lKaicteiydl",
        "outputId": "d5702706-b0bd-497a-9224-4fa91f518f72"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "449b7c3edcd340efa7f8246a374b216a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# #Load base/pretrained model for training\n",
        "\n",
        "# Clear GPU cache before loading the model for the second time\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Load model for training with CPU offloading enabled\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    # Enable CPU offloading for specific layers\n",
        "    llm_int8_enable_fp32_cpu_offload=True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",  # Let Transformers automatically decide device placement\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrceN0UzDnb6",
        "outputId": "36adf473-924e-4d4e-f629-ab8ddea67720"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gemma2ForCausalLM(\n",
            "  (model): Gemma2Model(\n",
            "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
            "    (layers): ModuleList(\n",
            "      (0-41): 42 x Gemma2DecoderLayer(\n",
            "        (self_attn): Gemma2Attention(\n",
            "          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
            "          (rotary_emb): Gemma2RotaryEmbedding()\n",
            "        )\n",
            "        (mlp): Gemma2MLP(\n",
            "          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
            "          (act_fn): PytorchGELUTanh()\n",
            "        )\n",
            "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
            "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
            "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
            "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"\"\"user: Generate summary of this dialogue in one line\n",
        "          dialogue:\n",
        "          Rachel:\n",
        "          Rachel: Top 50 Best Films of 2018\n",
        "          Rachel: :)\n",
        "          Janice: Omg, I've watched almost all 50... xDD\n",
        "          Spencer: Hahah, Deadpool 2 also??\n",
        "          Janice: Yep\n",
        "          Spencer: Really??\n",
        "          Janice: My bf forced me to watch it xD\n",
        "          Rachel: Hahah\n",
        "          Janice: It wasn't that bad\n",
        "          Janice: I thought it'd be worse\n",
        "          Rachel: And Avengers? :D\n",
        "          Janice: 2 times\n",
        "          Rachel: Omg\n",
        "          Janice: xP\n",
        "          Rachel: You are the best gf in the world\n",
        "          Rachel: Your bf should appreciate that ;-)\n",
        "          Janice: He does\n",
        "          Janice: x)\n",
        "AI Summary:\"\"\"\n",
        "\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")\n",
        "outputs = model.generate(**input_ids, max_length=228)\n",
        "\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kPxBcwuePX63",
        "outputId": "fae16ddf-9361-4ab1-a851-2090cd0c1756"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bos>user: Generate summary of this dialogue in one line\n",
            "          dialogue:\n",
            "          Rachel: \n",
            "          Rachel: Top 50 Best Films of 2018\n",
            "          Rachel: :)\n",
            "          Janice: Omg, I've watched almost all 50... xDD\n",
            "          Spencer: Hahah, Deadpool 2 also??\n",
            "          Janice: Yep\n",
            "          Spencer: Really??\n",
            "          Janice: My bf forced me to watch it xD\n",
            "          Rachel: Hahah\n",
            "          Janice: It wasn't that bad\n",
            "          Janice: I thought it'd be worse\n",
            "          Rachel: And Avengers? :D\n",
            "          Janice: 2 times\n",
            "          Rachel: Omg\n",
            "          Janice: xP\n",
            "          Rachel: You are the best gf in the world\n",
            "          Rachel: Your bf should appreciate that ;-)\n",
            "          Janice: He does\n",
            "          Janice: x)\n",
            "AI Summary: Janice and Rachel discuss Janice's extensive viewing of the Top 50 Best Films of 2018, including Deadpool 2 and Avengers. \n",
            "\n",
            "\n",
            "<end_of_turn><eos>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVHrGqI3ugDq"
      },
      "source": [
        "## **Distributed finetuning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7dUm1Q-cHc4S"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ya2UTpZbC6Jv"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "c_7ZQWEFDJcM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Clear GPU cache before loading the model for the second time\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Define LoRA configuration with the best hyperparameters\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.02,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "\n",
        "NUM_OF_ITERATION = 20\n",
        "\n",
        "# Define training arguments with the best hyperparameters\n",
        "training_arguments = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True, #<------\n",
        "    # num_train_epochs=NUM_OF_EPOCHS,\n",
        "    warmup_steps=2,\n",
        "    eval_strategy=\"steps\",  # \"epoch\", \"steps\",\n",
        "    eval_steps=0.2,\n",
        "    max_steps=NUM_OF_ITERATION,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=False,\n",
        "    bf16=False,\n",
        "    logging_steps=1,\n",
        "    output_dir=\"final_outputs\",\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "# training_arguments"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cutom optmizer for better memory utilization but not using in this code anymore\n",
        "'''\n",
        "import bitsandbytes as bnb\n",
        "from torch import nn\n",
        "from transformers.trainer_pt_utils import get_parameter_names\n",
        "\n",
        "training_args = transformers.TrainingArguments(per_device_train_batch_size= transformers.TrainingArguments.per_device_train_batch_size, output_dir=\"output\")\n",
        "\n",
        "decay_parameters = get_parameter_names(model, [nn.LayerNorm])\n",
        "decay_parameters = [name for name in decay_parameters if \"bias\" not in name]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n in decay_parameters],\n",
        "        \"weight_decay\": training_args.weight_decay,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if n not in decay_parameters],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer_kwargs = {\n",
        "    \"betas\": (training_args.adam_beta1, training_args.adam_beta2),\n",
        "    \"eps\": training_args.adam_epsilon,\n",
        "}\n",
        "optimizer_kwargs[\"lr\"] = training_args.learning_rate\n",
        "adam_bnb_optim = bnb.optim.Adam8bit(\n",
        "    optimizer_grouped_parameters,\n",
        "    betas=(training_args.adam_beta1, training_args.adam_beta2),\n",
        "    eps=training_args.adam_epsilon,\n",
        "    lr=training_args.learning_rate,\n",
        ")\n",
        "'''"
      ],
      "metadata": {
        "id": "vE1OShB5Czht"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPeXELCPC9pA",
        "outputId": "ddd3259f-5c94-4dae-849a-2b1015eceabb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "# Initialize the Accelerator\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# Ensure pad token is set\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # as it is a decoder-only model, it is recommended to set padding_side to \"left\".\n",
        "\n",
        "# Initialize the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=training_arguments.learning_rate)\n",
        "\n",
        "# #Prepare the model, tokenizer, datasets, and optimizer with the Accelerator\n",
        "# model, adam_bnb_optim, training_dataset, val_dataset = accelerator.prepare(\n",
        "#     model, adam_bnb_optim, training_dataset, val_dataset\n",
        "# )\n",
        "\n",
        "model, optimizer, training_dataset, val_dataset = accelerator.prepare(\n",
        "    model, optimizer, training_dataset, val_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "W2Rx40XbDYts",
        "outputId": "c75a14db-39dc-4510-871a-b1c6ecb3b178"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [20/20 07:05, Epoch 20/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.969900</td>\n",
              "      <td>2.269340</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.593700</td>\n",
              "      <td>2.213828</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.207600</td>\n",
              "      <td>2.259940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.881000</td>\n",
              "      <td>2.379813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.686900</td>\n",
              "      <td>2.459884</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "--->Execution Time: 450.19209814071655 seconds\n"
          ]
        }
      ],
      "source": [
        "from accelerate import DistributedType\n",
        "\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "# preprcessing before passing input\n",
        "def create_prompt(example):\n",
        "    text = f\"user:\\nSummarise dialogue in one sentence: {example['dialogue']} \\nSummary:\\n{example['summary']}\"\n",
        "    return [text]\n",
        "\n",
        "\n",
        "# Initialize Trainer with the best hyperparameters\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=training_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=lora_config,\n",
        "    max_seq_length=700,  # max length to input/output. It is crucial for GPU memory management\n",
        "    dataset_text_field=\"dialogue\",\n",
        "    formatting_func=create_prompt,  # preprocessing function before input\n",
        "    processing_class=tokenizer,\n",
        "    args=training_arguments,\n",
        "    packing=False,  # The trainer will attempt to pack multiple sequences into a single batch\n",
        ")\n",
        "\n",
        "# Train the final model\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Use the Accelerator to manage the training loop\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "# Save the final model\n",
        "# accelerator.wait_for_everyone() method is used to synchronize all processes in a distributed training setup,ensuring that all processes reach the same point before proceeding.\n",
        "# This is crucial for maintaining consistency and coordination across multiple devices (e.g., multiple GPUs or TPUs) during training.\n",
        "accelerator.wait_for_everyone()\n",
        "if accelerator.is_local_main_process:\n",
        "    trainer.model.save_pretrained(new_model)\n",
        "    tokenizer.save_pretrained(new_model)\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"\\n\\n--->Execution Time:\", end_time - start_time, \"seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge LORA finetuned model with base model"
      ],
      "metadata": {
        "id": "67No-gnilTkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "1rnbPuU4Y1Fb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import  PeftModel\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    # device_map=\"cpu\",\n",
        "    offload_folder=\"offload\",  # Specify offload folder\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "121262a7e919495b87062ecf053a0e54",
            "bab74325db964fc8b0820ffd01889f9b",
            "ca5e97efedc24e95afb55e8807e1c689",
            "108e357c560a46179f18d5aaa5416c7e",
            "f69581e5eba74c45a427554e22eb7c78",
            "c34d6f300ff041b8ac76d8dabc1248f3",
            "d48326708e674b1d8e60c59615f0dd58",
            "114b112518fd488dbd6f294a2c918f7c",
            "a631c9d7fdc340a79951dec87aef2c88",
            "3e16a4e2d14a4f9b95167430b2c2c713",
            "3b83d4059fbe4f22983669ec173997b6"
          ]
        },
        "id": "He1vDqscEily",
        "outputId": "b8dceb73-9f8e-46f1-ec20-194d6698fd4b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "121262a7e919495b87062ecf053a0e54"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear any cached GPU memory before loading PEFT model\n",
        "# torch.cuda.empty_cache()\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, new_model,offload_folder=\"offload_peft\" )\n",
        "# model = model.merge_and_unload()\n",
        "# with torch.no_grad():  # Disable gradient calculations to save memory\n",
        "#     model = model.merge_and_unload()\n",
        "\n",
        "# # Reload tokenizer to save it\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.padding_side = \"left\""
      ],
      "metadata": {
        "id": "fFPGU7CSL6VU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpGnV0ykc2OB",
        "outputId": "5cd6136e-53a4-46fb-b0ba-4512256cd3f4"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Gemma2ForCausalLM(\n",
              "      (model): Gemma2Model(\n",
              "        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
              "        (layers): ModuleList(\n",
              "          (0-41): 42 x Gemma2DecoderLayer(\n",
              "            (self_attn): Gemma2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.02, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.02, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=2048, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.02, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n",
              "              (rotary_emb): Gemma2RotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Gemma2MLP(\n",
              "              (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n",
              "              (act_fn): PytorchGELUTanh()\n",
              "            )\n",
              "            (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate\n",
        "\n",
        "```GPU out of memory exception during evaluation```"
      ],
      "metadata": {
        "id": "kxORZA8vlcYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "aoLL6rJOdWNV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"\"\"user: Generate summary of this dialogue in one line\n",
        "          dialogue:\n",
        "          Rachel:\n",
        "          Rachel: Top 50 Best Films of 2018\n",
        "          Rachel: :)\n",
        "          Janice: Omg, I've watched almost all 50... xDD\n",
        "          Spencer: Hahah, Deadpool 2 also??\n",
        "          Janice: Yep\n",
        "          Spencer: Really??\n",
        "          Janice: My bf forced me to watch it xD\n",
        "          Rachel: Hahah\n",
        "          Janice: It wasn't that bad\n",
        "          Janice: I thought it'd be worse\n",
        "          Rachel: And Avengers? :D\n",
        "          Janice: 2 times\n",
        "          Rachel: Omg\n",
        "          Janice: xP\n",
        "          Rachel: You are the best gf in the world\n",
        "          Rachel: Your bf should appreciate that ;-)\n",
        "          Janice: He does\n",
        "          Janice: x)\n",
        "AI Summary:\"\"\"\n",
        "\n",
        "# device = \"cuda:0\"\n",
        "# device = \"cpu\"\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "# model.to(device)\n",
        "\n",
        "\n",
        "# inputs = inputs.to(accelerator.device, dtype=torch.float16)\n",
        "# # # Move each tensor within the BatchEncoding to the accelerator's device and cast to float16\n",
        "# for key in inputs:\n",
        "#     # inputs[key] = inputs[key].to(accelerator.device, dtype=torch.float16)\n",
        "#     inputs[key] = inputs[key].to(dtype=torch.float16)\n",
        "\n",
        "true_summary = \"Rachel sends a list of Top 50 films of 2018. Janice watched almost half of them, Deadpool 2 and Avengers included.\"\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     outputs = model.generate(**inputs, max_length=288)\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=288)\n",
        "model_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(model_summary)\n",
        "\n",
        "print(\"---------------------------------------------------------------------\")\n",
        "end_token = \"\"\n",
        "\n",
        "highlight = str.strip(model_summary.split(\"AI Summary:\")[1])\n",
        "print(f\"Generated Summary: {highlight}\")\n",
        "print(\"---------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "zp0GZp3gSuPw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "0302ef49-adb6-486f-828d-6d2aed54f296"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 545434 has 14.73 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 286.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-e6cdde0b4150>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m#     outputs = model.generate(**inputs, max_length=288)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m288\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0mmodel_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_summary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1702\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1703\u001b[0m                     \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1704\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1705\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2082\u001b[0m         ):\n\u001b[1;32m   2083\u001b[0m             \u001b[0mmax_cache_length\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2084\u001b[0;31m         self._prepare_cache_for_generation(\n\u001b[0m\u001b[1;32m   2085\u001b[0m             \u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massistant_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_cache_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_cache_for_generation\u001b[0;34m(self, generation_config, model_kwargs, assistant_model, batch_size, max_cache_length, device)\u001b[0m\n\u001b[1;32m   1729\u001b[0m                         \u001b[0;34m\"issue: https://github.com/huggingface/transformers/issues/28981\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1730\u001b[0m                     )\n\u001b[0;32m-> 1731\u001b[0;31m                 model_kwargs[cache_name] = self._get_cache(\n\u001b[0m\u001b[1;32m   1732\u001b[0m                     \u001b[0mcache_implementation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_implementation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1733\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_get_cache\u001b[0;34m(self, cache_implementation, batch_size, max_cache_len, device, model_kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m                 \u001b[0;34m\"layer_device_map\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlayer_device_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m             }\n\u001b[0;32m-> 1637\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcache_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1638\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrequires_cross_attention_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1639\u001b[0m                 \u001b[0mencoder_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/cache_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, batch_size, max_cache_len, device, dtype, max_batch_size, layer_device_map)\u001b[0m\n\u001b[1;32m   1652\u001b[0m             \u001b[0;31m# breaks when updating the cache.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1653\u001b[0m             \u001b[0mcache_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_cache_shape\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sliding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msliding_cache_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1654\u001b[0;31m             \u001b[0mnew_layer_key_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1655\u001b[0m             \u001b[0mnew_layer_value_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_static_address\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_layer_key_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 11.06 MiB is free. Process 545434 has 14.73 GiB memory in use. Of the allocated memory 14.32 GiB is allocated by PyTorch, and 286.41 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VxmzMBa4_hiW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "449b7c3edcd340efa7f8246a374b216a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbf5052fed994b528114fe03bcaa39b9",
              "IPY_MODEL_997b3db3b16149bb948ee5724e803a7f",
              "IPY_MODEL_eb9fedf9246941d2b8330317cd5d3662"
            ],
            "layout": "IPY_MODEL_bcaa0b9d381249c78fb4a215e3929308"
          }
        },
        "fbf5052fed994b528114fe03bcaa39b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d89b7aaf89479c804ba706bd307017",
            "placeholder": "​",
            "style": "IPY_MODEL_30341ad92b7f4cc8a174f8b06c9d8ed7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "997b3db3b16149bb948ee5724e803a7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658e431aab0d47ffae04955ed3d9f4a7",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c497f05d31204d30b45e9fc009d79184",
            "value": 4
          }
        },
        "eb9fedf9246941d2b8330317cd5d3662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19cbb774e3ac4d7d9af51392a568e589",
            "placeholder": "​",
            "style": "IPY_MODEL_89815b317acd41a7a7afaa23eb022c7f",
            "value": " 4/4 [01:57&lt;00:00, 27.79s/it]"
          }
        },
        "bcaa0b9d381249c78fb4a215e3929308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40d89b7aaf89479c804ba706bd307017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30341ad92b7f4cc8a174f8b06c9d8ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658e431aab0d47ffae04955ed3d9f4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c497f05d31204d30b45e9fc009d79184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "19cbb774e3ac4d7d9af51392a568e589": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89815b317acd41a7a7afaa23eb022c7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "121262a7e919495b87062ecf053a0e54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bab74325db964fc8b0820ffd01889f9b",
              "IPY_MODEL_ca5e97efedc24e95afb55e8807e1c689",
              "IPY_MODEL_108e357c560a46179f18d5aaa5416c7e"
            ],
            "layout": "IPY_MODEL_f69581e5eba74c45a427554e22eb7c78"
          }
        },
        "bab74325db964fc8b0820ffd01889f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c34d6f300ff041b8ac76d8dabc1248f3",
            "placeholder": "​",
            "style": "IPY_MODEL_d48326708e674b1d8e60c59615f0dd58",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ca5e97efedc24e95afb55e8807e1c689": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_114b112518fd488dbd6f294a2c918f7c",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a631c9d7fdc340a79951dec87aef2c88",
            "value": 4
          }
        },
        "108e357c560a46179f18d5aaa5416c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e16a4e2d14a4f9b95167430b2c2c713",
            "placeholder": "​",
            "style": "IPY_MODEL_3b83d4059fbe4f22983669ec173997b6",
            "value": " 4/4 [01:50&lt;00:00, 26.66s/it]"
          }
        },
        "f69581e5eba74c45a427554e22eb7c78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c34d6f300ff041b8ac76d8dabc1248f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d48326708e674b1d8e60c59615f0dd58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "114b112518fd488dbd6f294a2c918f7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a631c9d7fdc340a79951dec87aef2c88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3e16a4e2d14a4f9b95167430b2c2c713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b83d4059fbe4f22983669ec173997b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}